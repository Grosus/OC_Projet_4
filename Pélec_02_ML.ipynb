{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import missingno as msno\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "from sklearn import decomposition\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, RobustScaler, OrdinalEncoder, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import r2_score , median_absolute_error, mean_absolute_error\n",
    "import shap\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour tenter de predire les variables TotalGHEEmission et SourceEUI nous allons mettre en place 5 modèles d'aprentissage supérvisé:\n",
    "Un modèle de régréssion linéaire qui nous servira de baseline\n",
    "Deux modèles linéaire (ElasticNet et Support Vector Regressor)\n",
    "Deux modèles d'arbres de décision (RandomForest et Xgboost)\n",
    "\n",
    "Chaque modèle sera entrainé en GridSearch sur different paramètres et nous verifieront les résultats avec trois métrique:\n",
    "Le r2\n",
    "la mean absolute error \n",
    "et la mean squared error\n",
    "\n",
    "De plus nous entraineront nos modèles sur 2 Set de données , l'un contenant la variable ENERGYSTARScore et l'autre non, cela permettra de conclure sur l'importance de cette variable pour nos prédictions, cependant pour eviter de surcharger ce Notebook l'entrainement du set donnée sans ENERGYSTARScore n'apparait pas et nous chargerons simplement les résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "noES_EUI_test_results=pd.read_csv('EUI_test_results.csv',sep='\\t')\n",
    "noES_GHG_test_results=pd.read_csv('GHG_test_results.csv',sep='\\t')\n",
    "noES_GHG_results=pd.read_csv('GHG_results.csv',sep='\\t')\n",
    "noES_EUI_results=pd.read_csv('EUI_results.csv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy=pd.read_csv('energy_clean.csv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OSEBuildingID</th>\n",
       "      <th>DataYear</th>\n",
       "      <th>BuildingType</th>\n",
       "      <th>PrimaryPropertyType</th>\n",
       "      <th>CouncilDistrictCode</th>\n",
       "      <th>Neighborhood</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>NumberofFloors</th>\n",
       "      <th>PropertyGFATotal</th>\n",
       "      <th>PropertyGFABuilding(s)</th>\n",
       "      <th>SecondLargestPropertyUseType</th>\n",
       "      <th>ENERGYSTARScore</th>\n",
       "      <th>SourceEUI(kBtu/sf)</th>\n",
       "      <th>SteamUse(kBtu)</th>\n",
       "      <th>NaturalGas(kBtu)</th>\n",
       "      <th>TotalGHGEmissions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2015</td>\n",
       "      <td>NONRESIDENTIAL</td>\n",
       "      <td>HOTEL</td>\n",
       "      <td>7</td>\n",
       "      <td>DOWNTOWN</td>\n",
       "      <td>1927</td>\n",
       "      <td>12.0</td>\n",
       "      <td>7369.500000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>HOTEL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>5.161925</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5.523179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2015</td>\n",
       "      <td>NONRESIDENTIAL</td>\n",
       "      <td>HOTEL</td>\n",
       "      <td>7</td>\n",
       "      <td>DOWNTOWN</td>\n",
       "      <td>1996</td>\n",
       "      <td>11.0</td>\n",
       "      <td>9415.090909</td>\n",
       "      <td>85.454686</td>\n",
       "      <td>PARKING</td>\n",
       "      <td>51.0</td>\n",
       "      <td>5.259057</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5.577879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2015</td>\n",
       "      <td>NONRESIDENTIAL</td>\n",
       "      <td>HOTEL</td>\n",
       "      <td>7</td>\n",
       "      <td>DOWNTOWN</td>\n",
       "      <td>1969</td>\n",
       "      <td>41.0</td>\n",
       "      <td>23463.170732</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>PARKING</td>\n",
       "      <td>18.0</td>\n",
       "      <td>5.495938</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7.631664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>2015</td>\n",
       "      <td>NONRESIDENTIAL</td>\n",
       "      <td>HOTEL</td>\n",
       "      <td>7</td>\n",
       "      <td>DOWNTOWN</td>\n",
       "      <td>1926</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6132.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>HOTEL</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.457241</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7.569071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>2015</td>\n",
       "      <td>NONRESIDENTIAL</td>\n",
       "      <td>HOTEL</td>\n",
       "      <td>7</td>\n",
       "      <td>DOWNTOWN</td>\n",
       "      <td>1980</td>\n",
       "      <td>18.0</td>\n",
       "      <td>6660.555556</td>\n",
       "      <td>89.607140</td>\n",
       "      <td>PARKING</td>\n",
       "      <td>67.0</td>\n",
       "      <td>5.437209</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.231858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2200</th>\n",
       "      <td>50069</td>\n",
       "      <td>2016</td>\n",
       "      <td>NONRESIDENTIAL</td>\n",
       "      <td>SMALL- AND MID-SIZED OFFICE</td>\n",
       "      <td>2</td>\n",
       "      <td>GREATER DUWAMISH</td>\n",
       "      <td>1929</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12495.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>SMALL- AND MID-SIZED OFFICE</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.908083</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.911183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2201</th>\n",
       "      <td>50081</td>\n",
       "      <td>2016</td>\n",
       "      <td>NONRESIDENTIAL</td>\n",
       "      <td>K-12 SCHOOL</td>\n",
       "      <td>3</td>\n",
       "      <td>GREATER DUWAMISH</td>\n",
       "      <td>2015</td>\n",
       "      <td>3.0</td>\n",
       "      <td>15000.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>PARKING</td>\n",
       "      <td>77.0</td>\n",
       "      <td>4.522875</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.326302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2202</th>\n",
       "      <td>50210</td>\n",
       "      <td>2016</td>\n",
       "      <td>NONRESIDENTIAL COS</td>\n",
       "      <td>OFFICE</td>\n",
       "      <td>7</td>\n",
       "      <td>MAGNOLIA / QUEEN ANNE</td>\n",
       "      <td>1952</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13661.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>OFFICE</td>\n",
       "      <td>75.0</td>\n",
       "      <td>4.757891</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.504077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2203</th>\n",
       "      <td>50220</td>\n",
       "      <td>2016</td>\n",
       "      <td>NONRESIDENTIAL COS</td>\n",
       "      <td>OFFICE</td>\n",
       "      <td>2</td>\n",
       "      <td>SOUTHEAST</td>\n",
       "      <td>1960</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15398.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>OFFICE</td>\n",
       "      <td>93.0</td>\n",
       "      <td>4.175925</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.173615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2204</th>\n",
       "      <td>50222</td>\n",
       "      <td>2016</td>\n",
       "      <td>NONRESIDENTIAL COS</td>\n",
       "      <td>OFFICE</td>\n",
       "      <td>2</td>\n",
       "      <td>GREATER DUWAMISH</td>\n",
       "      <td>1990</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12294.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>OFFICE</td>\n",
       "      <td>46.0</td>\n",
       "      <td>5.091908</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.088311</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2205 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      OSEBuildingID  DataYear        BuildingType  \\\n",
       "0                 1      2015      NONRESIDENTIAL   \n",
       "1                 2      2015      NONRESIDENTIAL   \n",
       "2                 3      2015      NONRESIDENTIAL   \n",
       "3                 5      2015      NONRESIDENTIAL   \n",
       "4                 8      2015      NONRESIDENTIAL   \n",
       "...             ...       ...                 ...   \n",
       "2200          50069      2016      NONRESIDENTIAL   \n",
       "2201          50081      2016      NONRESIDENTIAL   \n",
       "2202          50210      2016  NONRESIDENTIAL COS   \n",
       "2203          50220      2016  NONRESIDENTIAL COS   \n",
       "2204          50222      2016  NONRESIDENTIAL COS   \n",
       "\n",
       "              PrimaryPropertyType  CouncilDistrictCode           Neighborhood  \\\n",
       "0                           HOTEL                    7               DOWNTOWN   \n",
       "1                           HOTEL                    7               DOWNTOWN   \n",
       "2                           HOTEL                    7               DOWNTOWN   \n",
       "3                           HOTEL                    7               DOWNTOWN   \n",
       "4                           HOTEL                    7               DOWNTOWN   \n",
       "...                           ...                  ...                    ...   \n",
       "2200  SMALL- AND MID-SIZED OFFICE                    2       GREATER DUWAMISH   \n",
       "2201                  K-12 SCHOOL                    3       GREATER DUWAMISH   \n",
       "2202                       OFFICE                    7  MAGNOLIA / QUEEN ANNE   \n",
       "2203                       OFFICE                    2              SOUTHEAST   \n",
       "2204                       OFFICE                    2       GREATER DUWAMISH   \n",
       "\n",
       "      YearBuilt  NumberofFloors  PropertyGFATotal  PropertyGFABuilding(s)  \\\n",
       "0          1927            12.0       7369.500000              100.000000   \n",
       "1          1996            11.0       9415.090909               85.454686   \n",
       "2          1969            41.0      23463.170732              100.000000   \n",
       "3          1926            10.0       6132.000000              100.000000   \n",
       "4          1980            18.0       6660.555556               89.607140   \n",
       "...         ...             ...               ...                     ...   \n",
       "2200       1929             2.0      12495.000000              100.000000   \n",
       "2201       2015             3.0      15000.000000              100.000000   \n",
       "2202       1952             1.0      13661.000000              100.000000   \n",
       "2203       1960             1.0      15398.000000              100.000000   \n",
       "2204       1990             1.0      12294.000000              100.000000   \n",
       "\n",
       "     SecondLargestPropertyUseType  ENERGYSTARScore  SourceEUI(kBtu/sf)  \\\n",
       "0                           HOTEL             65.0            5.161925   \n",
       "1                         PARKING             51.0            5.259057   \n",
       "2                         PARKING             18.0            5.495938   \n",
       "3                           HOTEL              1.0            6.457241   \n",
       "4                         PARKING             67.0            5.437209   \n",
       "...                           ...              ...                 ...   \n",
       "2200  SMALL- AND MID-SIZED OFFICE              9.0            5.908083   \n",
       "2201                      PARKING             77.0            4.522875   \n",
       "2202                       OFFICE             75.0            4.757891   \n",
       "2203                       OFFICE             93.0            4.175925   \n",
       "2204                       OFFICE             46.0            5.091908   \n",
       "\n",
       "      SteamUse(kBtu)  NaturalGas(kBtu)  TotalGHGEmissions  \n",
       "0                  1                 1           5.523179  \n",
       "1                  0                 1           5.577879  \n",
       "2                  1                 1           7.631664  \n",
       "3                  1                 1           7.569071  \n",
       "4                  0                 1           6.231858  \n",
       "...              ...               ...                ...  \n",
       "2200               0                 1           4.911183  \n",
       "2201               0                 0           2.326302  \n",
       "2202               0                 0           1.504077  \n",
       "2203               0                 1           2.173615  \n",
       "2204               0                 1           3.088311  \n",
       "\n",
       "[2205 rows x 16 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2205 entries, 0 to 2204\n",
      "Data columns (total 16 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   OSEBuildingID                 2205 non-null   int64  \n",
      " 1   DataYear                      2205 non-null   int64  \n",
      " 2   BuildingType                  2205 non-null   object \n",
      " 3   PrimaryPropertyType           2205 non-null   object \n",
      " 4   CouncilDistrictCode           2205 non-null   int64  \n",
      " 5   Neighborhood                  2205 non-null   object \n",
      " 6   YearBuilt                     2205 non-null   int64  \n",
      " 7   NumberofFloors                2205 non-null   float64\n",
      " 8   PropertyGFATotal              2205 non-null   float64\n",
      " 9   PropertyGFABuilding(s)        2205 non-null   float64\n",
      " 10  SecondLargestPropertyUseType  2205 non-null   object \n",
      " 11  ENERGYSTARScore               2205 non-null   float64\n",
      " 12  SourceEUI(kBtu/sf)            2205 non-null   float64\n",
      " 13  SteamUse(kBtu)                2205 non-null   int64  \n",
      " 14  NaturalGas(kBtu)              2205 non-null   int64  \n",
      " 15  TotalGHGEmissions             2205 non-null   float64\n",
      "dtypes: float64(6), int64(6), object(4)\n",
      "memory usage: 275.8+ KB\n"
     ]
    }
   ],
   "source": [
    "energy.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n"
     ]
    }
   ],
   "source": [
    "energy=energy.drop('OSEBuildingID',1)\n",
    "numeric = ['NumberofFloors','PropertyGFATotal', 'PropertyGFABuilding(s)','YearBuilt','ENERGYSTARScore','CouncilDistrictCode']\n",
    "ordinal=['BuildingType','Neighborhood','PrimaryPropertyType','SecondLargestPropertyUseType']\n",
    "binar=['SteamUse(kBtu)','NaturalGas(kBtu)']\n",
    "target=[\"TotalGHGEmissions\",'SourceEUI(kBtu/sf)']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DataYear</th>\n",
       "      <th>BuildingType</th>\n",
       "      <th>PrimaryPropertyType</th>\n",
       "      <th>CouncilDistrictCode</th>\n",
       "      <th>Neighborhood</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>NumberofFloors</th>\n",
       "      <th>PropertyGFATotal</th>\n",
       "      <th>PropertyGFABuilding(s)</th>\n",
       "      <th>SecondLargestPropertyUseType</th>\n",
       "      <th>ENERGYSTARScore</th>\n",
       "      <th>SourceEUI(kBtu/sf)</th>\n",
       "      <th>SteamUse(kBtu)</th>\n",
       "      <th>NaturalGas(kBtu)</th>\n",
       "      <th>TotalGHGEmissions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015</td>\n",
       "      <td>NONRESIDENTIAL</td>\n",
       "      <td>HOTEL</td>\n",
       "      <td>7</td>\n",
       "      <td>DOWNTOWN</td>\n",
       "      <td>1927</td>\n",
       "      <td>12.0</td>\n",
       "      <td>7369.500000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>HOTEL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>5.161925</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5.523179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015</td>\n",
       "      <td>NONRESIDENTIAL</td>\n",
       "      <td>HOTEL</td>\n",
       "      <td>7</td>\n",
       "      <td>DOWNTOWN</td>\n",
       "      <td>1996</td>\n",
       "      <td>11.0</td>\n",
       "      <td>9415.090909</td>\n",
       "      <td>85.454686</td>\n",
       "      <td>PARKING</td>\n",
       "      <td>51.0</td>\n",
       "      <td>5.259057</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5.577879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015</td>\n",
       "      <td>NONRESIDENTIAL</td>\n",
       "      <td>HOTEL</td>\n",
       "      <td>7</td>\n",
       "      <td>DOWNTOWN</td>\n",
       "      <td>1969</td>\n",
       "      <td>41.0</td>\n",
       "      <td>23463.170732</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>PARKING</td>\n",
       "      <td>18.0</td>\n",
       "      <td>5.495938</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7.631664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015</td>\n",
       "      <td>NONRESIDENTIAL</td>\n",
       "      <td>HOTEL</td>\n",
       "      <td>7</td>\n",
       "      <td>DOWNTOWN</td>\n",
       "      <td>1926</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6132.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>HOTEL</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.457241</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7.569071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015</td>\n",
       "      <td>NONRESIDENTIAL</td>\n",
       "      <td>HOTEL</td>\n",
       "      <td>7</td>\n",
       "      <td>DOWNTOWN</td>\n",
       "      <td>1980</td>\n",
       "      <td>18.0</td>\n",
       "      <td>6660.555556</td>\n",
       "      <td>89.607140</td>\n",
       "      <td>PARKING</td>\n",
       "      <td>67.0</td>\n",
       "      <td>5.437209</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.231858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2200</th>\n",
       "      <td>2016</td>\n",
       "      <td>NONRESIDENTIAL</td>\n",
       "      <td>SMALL- AND MID-SIZED OFFICE</td>\n",
       "      <td>2</td>\n",
       "      <td>GREATER DUWAMISH</td>\n",
       "      <td>1929</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12495.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>SMALL- AND MID-SIZED OFFICE</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.908083</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.911183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2201</th>\n",
       "      <td>2016</td>\n",
       "      <td>NONRESIDENTIAL</td>\n",
       "      <td>K-12 SCHOOL</td>\n",
       "      <td>3</td>\n",
       "      <td>GREATER DUWAMISH</td>\n",
       "      <td>2015</td>\n",
       "      <td>3.0</td>\n",
       "      <td>15000.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>PARKING</td>\n",
       "      <td>77.0</td>\n",
       "      <td>4.522875</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.326302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2202</th>\n",
       "      <td>2016</td>\n",
       "      <td>NONRESIDENTIAL COS</td>\n",
       "      <td>OFFICE</td>\n",
       "      <td>7</td>\n",
       "      <td>MAGNOLIA / QUEEN ANNE</td>\n",
       "      <td>1952</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13661.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>OFFICE</td>\n",
       "      <td>75.0</td>\n",
       "      <td>4.757891</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.504077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2203</th>\n",
       "      <td>2016</td>\n",
       "      <td>NONRESIDENTIAL COS</td>\n",
       "      <td>OFFICE</td>\n",
       "      <td>2</td>\n",
       "      <td>SOUTHEAST</td>\n",
       "      <td>1960</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15398.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>OFFICE</td>\n",
       "      <td>93.0</td>\n",
       "      <td>4.175925</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.173615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2204</th>\n",
       "      <td>2016</td>\n",
       "      <td>NONRESIDENTIAL COS</td>\n",
       "      <td>OFFICE</td>\n",
       "      <td>2</td>\n",
       "      <td>GREATER DUWAMISH</td>\n",
       "      <td>1990</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12294.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>OFFICE</td>\n",
       "      <td>46.0</td>\n",
       "      <td>5.091908</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.088311</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2205 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      DataYear        BuildingType          PrimaryPropertyType  \\\n",
       "0         2015      NONRESIDENTIAL                        HOTEL   \n",
       "1         2015      NONRESIDENTIAL                        HOTEL   \n",
       "2         2015      NONRESIDENTIAL                        HOTEL   \n",
       "3         2015      NONRESIDENTIAL                        HOTEL   \n",
       "4         2015      NONRESIDENTIAL                        HOTEL   \n",
       "...        ...                 ...                          ...   \n",
       "2200      2016      NONRESIDENTIAL  SMALL- AND MID-SIZED OFFICE   \n",
       "2201      2016      NONRESIDENTIAL                  K-12 SCHOOL   \n",
       "2202      2016  NONRESIDENTIAL COS                       OFFICE   \n",
       "2203      2016  NONRESIDENTIAL COS                       OFFICE   \n",
       "2204      2016  NONRESIDENTIAL COS                       OFFICE   \n",
       "\n",
       "      CouncilDistrictCode           Neighborhood  YearBuilt  NumberofFloors  \\\n",
       "0                       7               DOWNTOWN       1927            12.0   \n",
       "1                       7               DOWNTOWN       1996            11.0   \n",
       "2                       7               DOWNTOWN       1969            41.0   \n",
       "3                       7               DOWNTOWN       1926            10.0   \n",
       "4                       7               DOWNTOWN       1980            18.0   \n",
       "...                   ...                    ...        ...             ...   \n",
       "2200                    2       GREATER DUWAMISH       1929             2.0   \n",
       "2201                    3       GREATER DUWAMISH       2015             3.0   \n",
       "2202                    7  MAGNOLIA / QUEEN ANNE       1952             1.0   \n",
       "2203                    2              SOUTHEAST       1960             1.0   \n",
       "2204                    2       GREATER DUWAMISH       1990             1.0   \n",
       "\n",
       "      PropertyGFATotal  PropertyGFABuilding(s) SecondLargestPropertyUseType  \\\n",
       "0          7369.500000              100.000000                        HOTEL   \n",
       "1          9415.090909               85.454686                      PARKING   \n",
       "2         23463.170732              100.000000                      PARKING   \n",
       "3          6132.000000              100.000000                        HOTEL   \n",
       "4          6660.555556               89.607140                      PARKING   \n",
       "...                ...                     ...                          ...   \n",
       "2200      12495.000000              100.000000  SMALL- AND MID-SIZED OFFICE   \n",
       "2201      15000.000000              100.000000                      PARKING   \n",
       "2202      13661.000000              100.000000                       OFFICE   \n",
       "2203      15398.000000              100.000000                       OFFICE   \n",
       "2204      12294.000000              100.000000                       OFFICE   \n",
       "\n",
       "      ENERGYSTARScore  SourceEUI(kBtu/sf)  SteamUse(kBtu)  NaturalGas(kBtu)  \\\n",
       "0                65.0            5.161925               1                 1   \n",
       "1                51.0            5.259057               0                 1   \n",
       "2                18.0            5.495938               1                 1   \n",
       "3                 1.0            6.457241               1                 1   \n",
       "4                67.0            5.437209               0                 1   \n",
       "...               ...                 ...             ...               ...   \n",
       "2200              9.0            5.908083               0                 1   \n",
       "2201             77.0            4.522875               0                 0   \n",
       "2202             75.0            4.757891               0                 0   \n",
       "2203             93.0            4.175925               0                 1   \n",
       "2204             46.0            5.091908               0                 1   \n",
       "\n",
       "      TotalGHGEmissions  \n",
       "0              5.523179  \n",
       "1              5.577879  \n",
       "2              7.631664  \n",
       "3              7.569071  \n",
       "4              6.231858  \n",
       "...                 ...  \n",
       "2200           4.911183  \n",
       "2201           2.326302  \n",
       "2202           1.504077  \n",
       "2203           2.173615  \n",
       "2204           3.088311  \n",
       "\n",
       "[2205 rows x 15 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "energy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apres avoir utiliser different preprocessing pour entrainer mes modèles , j'ai decidé d'opter les transformers suivant qui apportent de meilleurs résultats sur les meilleurs modèles:\n",
    "un OrdinalEncoder pour les variables catégorielle\n",
    "un RobustScaler pour les variables numérique\n",
    "les variables 'SteamUse(kBtu)' et 'NaturalGas(kBtu)' étant deja binaire elles ne seront pas transformé\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('ordinal',OrdinalEncoder(handle_unknown=\"use_encoded_value\",unknown_value = -1) , ordinal),\n",
    "    ('numeric',RobustScaler(), numeric),\n",
    "    ('binar','passthrough',binar)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retourne les erreurs obtenue pour les meilleurs paramètre d'un modèle durant la cross validation\n",
    "def scores(model,step):\n",
    "    temp = pd.DataFrame.from_dict(model.named_steps[step].cv_results_)\n",
    "    best_params = model.named_steps[step].best_params_\n",
    "    temp=temp[temp['params']==best_params]\n",
    "    temp.drop(list(temp.filter(regex = 'param_')), axis = 1, inplace = True)\n",
    "    return temp\n",
    "\n",
    "#Retourne les erreurs obtenue pour les meilleurs paramètre d'un modèle lors de la prediction des données de test\n",
    "def test_scores(name,model,X_test,Y_true):\n",
    "    Y_pred=model.predict(X_test)\n",
    "    test=[name]\n",
    "    test.append(r2_score(Y_true, Y_pred))\n",
    "    test.append(median_absolute_error(Y_true, Y_pred))\n",
    "    test.append(mean_absolute_error(Y_true, Y_pred))\n",
    "    ttest=pd.DataFrame([test],columns=['name', 'test_r2', 'test_median_abs_error','test-mean-absolute_error'])\n",
    "    return(ttest)\n",
    "    \n",
    "GHG_test_results=pd.DataFrame(columns=['name', 'test_r2', 'test_median_abs_error','test-mean-absolute_error'])\n",
    "EUI_test_results=pd.DataFrame(columns=['name', 'test_r2', 'test_median_abs_error','test-mean-absolute_error'])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noues commencons par séparer notre DataFrame en deux partie , un train set et un test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = energy.drop([\"TotalGHGEmissions\",'SourceEUI(kBtu/sf)','DataYear'], axis=1)\n",
    "Y = energy[[\"TotalGHGEmissions\",'SourceEUI(kBtu/sf)']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression lineaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il s'agit d'un modele de régréssion linéaire classique qui nous servira de Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_mlr = {\"fit_intercept\": [True, False]}\n",
    "\n",
    "mlr_grid_cv = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('grid_search_mlr', GridSearchCV(LinearRegression(),\n",
    "                            param_grid=param_mlr,\n",
    "                            cv=5,\n",
    "                            scoring=('r2','neg_mean_absolute_error','neg_mean_squared_error'),\n",
    "                            return_train_score = True,\n",
    "                            refit='neg_mean_absolute_error',\n",
    "                            n_jobs = -1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ajout des métriques de Cross Validation dans le Dataframe de résultats\n",
    "GHG_mlr_model=mlr_grid_cv.fit(X_train, Y_train[\"TotalGHGEmissions\"])\n",
    "GHG_results =scores(GHG_mlr_model,'grid_search_mlr')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ajout des métriques de Cross Validation dans le Dataframe de résultats\n",
    "EUI_mlr_model=mlr_grid_cv.fit(X_train, Y_train['SourceEUI(kBtu/sf)'])\n",
    "EUI_results =scores(EUI_mlr_model,'grid_search_mlr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ajout des métriques de test dans le Dataframe de résultats\n",
    "GHG_test_results=pd.concat([GHG_test_results,test_scores('Regression linéaire',GHG_mlr_model,X_test,Y_test[\"TotalGHGEmissions\"])])\n",
    "EUI_test_results=pd.concat([EUI_test_results,test_scores('Regression linéaire',EUI_mlr_model,X_test,Y_test['SourceEUI(kBtu/sf)'])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>test_r2</th>\n",
       "      <th>test_median_abs_error</th>\n",
       "      <th>test-mean-absolute_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Regression linéaire</td>\n",
       "      <td>-0.086697</td>\n",
       "      <td>1.011104</td>\n",
       "      <td>1.200885</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  name   test_r2 test_median_abs_error  \\\n",
       "0  Regression linéaire -0.086697              1.011104   \n",
       "\n",
       "  test-mean-absolute_error  \n",
       "0                 1.200885  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GHG_test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elastic Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il s'agit d'une regréssion linéaire classique a laquelle on ajuste une combinaison des régulateurs l1 (Lasso) et l2 (Ridge) afin d'ajouter un biais sur certaines valeurs  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "param_eNet = {\"max_iter\": [10, 100, 1000],\n",
    "              \"alpha\": np.logspace(-4, 0, num=5),\n",
    "              \"l1_ratio\": np.arange(0.0, 1.1, 0.1)}\n",
    "\n",
    "eNet_grid_cv = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('grid_search_enet', GridSearchCV(ElasticNet(),\n",
    "                            param_grid=param_eNet,\n",
    "                            cv=5,\n",
    "                            scoring=('r2','neg_mean_absolute_error','neg_mean_squared_error'),\n",
    "                            return_train_score = True,\n",
    "                            refit='neg_mean_absolute_error',\n",
    "                            n_jobs = -1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "GHG_eNet_model=eNet_grid_cv.fit(X_train, Y_train[\"TotalGHGEmissions\"])\n",
    "GHG_results=pd.concat([GHG_results, scores(GHG_eNet_model,'grid_search_enet')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.302e-01, tolerance: 1.039e-01\n"
     ]
    }
   ],
   "source": [
    "EUI_eNet_model=eNet_grid_cv.fit(X_train, Y_train['SourceEUI(kBtu/sf)'])\n",
    "EUI_results=pd.concat([EUI_results, scores(EUI_eNet_model,'grid_search_enet')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_r2</th>\n",
       "      <th>split1_test_r2</th>\n",
       "      <th>split2_test_r2</th>\n",
       "      <th>split3_test_r2</th>\n",
       "      <th>split4_test_r2</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_neg_mean_squared_error</th>\n",
       "      <th>std_test_neg_mean_squared_error</th>\n",
       "      <th>rank_test_neg_mean_squared_error</th>\n",
       "      <th>split0_train_neg_mean_squared_error</th>\n",
       "      <th>split1_train_neg_mean_squared_error</th>\n",
       "      <th>split2_train_neg_mean_squared_error</th>\n",
       "      <th>split3_train_neg_mean_squared_error</th>\n",
       "      <th>split4_train_neg_mean_squared_error</th>\n",
       "      <th>mean_train_neg_mean_squared_error</th>\n",
       "      <th>std_train_neg_mean_squared_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.006801</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>0.001201</td>\n",
       "      <td>4.005194e-04</td>\n",
       "      <td>{'fit_intercept': True}</td>\n",
       "      <td>0.610725</td>\n",
       "      <td>0.632993</td>\n",
       "      <td>0.508543</td>\n",
       "      <td>0.563661</td>\n",
       "      <td>0.617712</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.796764</td>\n",
       "      <td>0.060304</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.781997</td>\n",
       "      <td>-0.792184</td>\n",
       "      <td>-0.751358</td>\n",
       "      <td>-0.771171</td>\n",
       "      <td>-0.763948</td>\n",
       "      <td>-0.772132</td>\n",
       "      <td>0.014134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>3.162980e-07</td>\n",
       "      <td>{'alpha': 0.01, 'l1_ratio': 0.2, 'max_iter': 10}</td>\n",
       "      <td>0.609197</td>\n",
       "      <td>0.630209</td>\n",
       "      <td>0.509467</td>\n",
       "      <td>0.556276</td>\n",
       "      <td>0.616000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.801540</td>\n",
       "      <td>0.058381</td>\n",
       "      <td>91</td>\n",
       "      <td>-0.785689</td>\n",
       "      <td>-0.796043</td>\n",
       "      <td>-0.755287</td>\n",
       "      <td>-0.774685</td>\n",
       "      <td>-0.767863</td>\n",
       "      <td>-0.775913</td>\n",
       "      <td>0.014089</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0        0.006801        0.0004         0.001201    4.005194e-04   \n",
       "72       0.000800        0.0004         0.001000    3.162980e-07   \n",
       "\n",
       "                                              params  split0_test_r2  \\\n",
       "0                            {'fit_intercept': True}        0.610725   \n",
       "72  {'alpha': 0.01, 'l1_ratio': 0.2, 'max_iter': 10}        0.609197   \n",
       "\n",
       "    split1_test_r2  split2_test_r2  split3_test_r2  split4_test_r2  ...  \\\n",
       "0         0.632993        0.508543        0.563661        0.617712  ...   \n",
       "72        0.630209        0.509467        0.556276        0.616000  ...   \n",
       "\n",
       "    mean_test_neg_mean_squared_error  std_test_neg_mean_squared_error  \\\n",
       "0                          -0.796764                         0.060304   \n",
       "72                         -0.801540                         0.058381   \n",
       "\n",
       "    rank_test_neg_mean_squared_error  split0_train_neg_mean_squared_error  \\\n",
       "0                                  1                            -0.781997   \n",
       "72                                91                            -0.785689   \n",
       "\n",
       "    split1_train_neg_mean_squared_error  split2_train_neg_mean_squared_error  \\\n",
       "0                             -0.792184                            -0.751358   \n",
       "72                            -0.796043                            -0.755287   \n",
       "\n",
       "    split3_train_neg_mean_squared_error  split4_train_neg_mean_squared_error  \\\n",
       "0                             -0.771171                            -0.763948   \n",
       "72                            -0.774685                            -0.767863   \n",
       "\n",
       "    mean_train_neg_mean_squared_error  std_train_neg_mean_squared_error  \n",
       "0                           -0.772132                          0.014134  \n",
       "72                          -0.775913                          0.014089  \n",
       "\n",
       "[2 rows x 50 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GHG_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "GHG_test_results=pd.concat([GHG_test_results,test_scores('ElasticNet',GHG_eNet_model,X_test,Y_test[\"TotalGHGEmissions\"])])\n",
    "EUI_test_results=pd.concat([EUI_test_results,test_scores('ElasticNet',EUI_eNet_model,X_test,Y_test['SourceEUI(kBtu/sf)'])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il s'agit d'un modèle linéaire qui consiste à decouper notre espace avec des hyperplan afin d'estimer les prédictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "param_svr = {'C' : np.logspace(-4, 0, 5),\n",
    "             'epsilon' : [0, 0.01, 0.1, 0.5, 1, 2],\n",
    "             'loss' : [\"epsilon_insensitive\",\"squared_epsilon_insensitive\"],\n",
    "             'max_iter': [10, 100, 1000]}\n",
    "\n",
    "svr_grid_cv = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('grid_search_svr', GridSearchCV(LinearSVR(),param_grid=param_svr,\n",
    "                            cv=5,\n",
    "                            scoring=('r2','neg_mean_absolute_error','neg_mean_squared_error'),\n",
    "                            refit='neg_mean_absolute_error',\n",
    "                            return_train_score = True,\n",
    "                            n_jobs = -1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Liblinear failed to converge, increase the number of iterations.\n"
     ]
    }
   ],
   "source": [
    "GHG_svr_model=svr_grid_cv.fit(X_train, Y_train[\"TotalGHGEmissions\"])\n",
    "GHG_results=pd.concat([GHG_results, scores(GHG_svr_model,'grid_search_svr')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Liblinear failed to converge, increase the number of iterations.\n"
     ]
    }
   ],
   "source": [
    "EUI_svr_model=svr_grid_cv.fit(X_train, Y_train['SourceEUI(kBtu/sf)'])\n",
    "EUI_results=pd.concat([EUI_results, scores(EUI_svr_model,'grid_search_svr')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_r2</th>\n",
       "      <th>split1_test_r2</th>\n",
       "      <th>split2_test_r2</th>\n",
       "      <th>split3_test_r2</th>\n",
       "      <th>split4_test_r2</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_neg_mean_squared_error</th>\n",
       "      <th>std_test_neg_mean_squared_error</th>\n",
       "      <th>rank_test_neg_mean_squared_error</th>\n",
       "      <th>split0_train_neg_mean_squared_error</th>\n",
       "      <th>split1_train_neg_mean_squared_error</th>\n",
       "      <th>split2_train_neg_mean_squared_error</th>\n",
       "      <th>split3_train_neg_mean_squared_error</th>\n",
       "      <th>split4_train_neg_mean_squared_error</th>\n",
       "      <th>mean_train_neg_mean_squared_error</th>\n",
       "      <th>std_train_neg_mean_squared_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.006801</td>\n",
       "      <td>0.00040</td>\n",
       "      <td>0.001201</td>\n",
       "      <td>4.005194e-04</td>\n",
       "      <td>{'fit_intercept': True}</td>\n",
       "      <td>0.610725</td>\n",
       "      <td>0.632993</td>\n",
       "      <td>0.508543</td>\n",
       "      <td>0.563661</td>\n",
       "      <td>0.617712</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.796764</td>\n",
       "      <td>0.060304</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.781997</td>\n",
       "      <td>-0.792184</td>\n",
       "      <td>-0.751358</td>\n",
       "      <td>-0.771171</td>\n",
       "      <td>-0.763948</td>\n",
       "      <td>-0.772132</td>\n",
       "      <td>0.014134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.00040</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>3.162980e-07</td>\n",
       "      <td>{'alpha': 0.01, 'l1_ratio': 0.2, 'max_iter': 10}</td>\n",
       "      <td>0.609197</td>\n",
       "      <td>0.630209</td>\n",
       "      <td>0.509467</td>\n",
       "      <td>0.556276</td>\n",
       "      <td>0.616000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.801540</td>\n",
       "      <td>0.058381</td>\n",
       "      <td>91</td>\n",
       "      <td>-0.785689</td>\n",
       "      <td>-0.796043</td>\n",
       "      <td>-0.755287</td>\n",
       "      <td>-0.774685</td>\n",
       "      <td>-0.767863</td>\n",
       "      <td>-0.775913</td>\n",
       "      <td>0.014089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>0.089220</td>\n",
       "      <td>0.00768</td>\n",
       "      <td>0.001001</td>\n",
       "      <td>3.234067e-07</td>\n",
       "      <td>{'C': 0.1, 'epsilon': 0, 'loss': 'squared_epsi...</td>\n",
       "      <td>0.613054</td>\n",
       "      <td>0.625338</td>\n",
       "      <td>0.508194</td>\n",
       "      <td>0.561702</td>\n",
       "      <td>0.613003</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.801711</td>\n",
       "      <td>0.058311</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.785084</td>\n",
       "      <td>-0.795427</td>\n",
       "      <td>-0.755179</td>\n",
       "      <td>-0.774191</td>\n",
       "      <td>-0.766426</td>\n",
       "      <td>-0.775261</td>\n",
       "      <td>0.014040</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0         0.006801       0.00040         0.001201    4.005194e-04   \n",
       "72        0.000800       0.00040         0.001000    3.162980e-07   \n",
       "113       0.089220       0.00768         0.001001    3.234067e-07   \n",
       "\n",
       "                                                params  split0_test_r2  \\\n",
       "0                              {'fit_intercept': True}        0.610725   \n",
       "72    {'alpha': 0.01, 'l1_ratio': 0.2, 'max_iter': 10}        0.609197   \n",
       "113  {'C': 0.1, 'epsilon': 0, 'loss': 'squared_epsi...        0.613054   \n",
       "\n",
       "     split1_test_r2  split2_test_r2  split3_test_r2  split4_test_r2  ...  \\\n",
       "0          0.632993        0.508543        0.563661        0.617712  ...   \n",
       "72         0.630209        0.509467        0.556276        0.616000  ...   \n",
       "113        0.625338        0.508194        0.561702        0.613003  ...   \n",
       "\n",
       "     mean_test_neg_mean_squared_error  std_test_neg_mean_squared_error  \\\n",
       "0                           -0.796764                         0.060304   \n",
       "72                          -0.801540                         0.058381   \n",
       "113                         -0.801711                         0.058311   \n",
       "\n",
       "     rank_test_neg_mean_squared_error  split0_train_neg_mean_squared_error  \\\n",
       "0                                   1                            -0.781997   \n",
       "72                                 91                            -0.785689   \n",
       "113                                 2                            -0.785084   \n",
       "\n",
       "     split1_train_neg_mean_squared_error  split2_train_neg_mean_squared_error  \\\n",
       "0                              -0.792184                            -0.751358   \n",
       "72                             -0.796043                            -0.755287   \n",
       "113                            -0.795427                            -0.755179   \n",
       "\n",
       "     split3_train_neg_mean_squared_error  split4_train_neg_mean_squared_error  \\\n",
       "0                              -0.771171                            -0.763948   \n",
       "72                             -0.774685                            -0.767863   \n",
       "113                            -0.774191                            -0.766426   \n",
       "\n",
       "     mean_train_neg_mean_squared_error  std_train_neg_mean_squared_error  \n",
       "0                            -0.772132                          0.014134  \n",
       "72                           -0.775913                          0.014089  \n",
       "113                          -0.775261                          0.014040  \n",
       "\n",
       "[3 rows x 50 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GHG_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "GHG_test_results=pd.concat([GHG_test_results,test_scores('Support Vector',GHG_svr_model,X_test,Y_test[\"TotalGHGEmissions\"])])\n",
    "EUI_test_results=pd.concat([EUI_test_results,test_scores('Support Vector',EUI_svr_model,X_test,Y_test['SourceEUI(kBtu/sf)'])])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il s'agit d'un modele qui créer des arbres de décision indépendant et les assembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "param_rfr = {'max_features' : ['sqrt', 'log2'],\n",
    "             'max_depth': [5, 15, 25, 50],\n",
    "             'min_samples_split': [2, 5, 10],\n",
    "             'bootstrap' : [True, False],\n",
    "             'min_samples_leaf': [1,2,5,10]}\n",
    "\n",
    "rfr_grid_cv = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('grid_search_rfr', GridSearchCV(RandomForestRegressor(),param_grid=param_rfr,\n",
    "                            cv=5,\n",
    "                            scoring=('r2','neg_mean_absolute_error','neg_mean_squared_error'),\n",
    "                            refit='neg_mean_absolute_error',\n",
    "                            return_train_score = True,\n",
    "                            n_jobs = -1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['BuildingType', 'PrimaryPropertyType', 'CouncilDistrictCode',\n",
       "       'Neighborhood', 'YearBuilt', 'NumberofFloors', 'PropertyGFATotal',\n",
       "       'PropertyGFABuilding(s)', 'SecondLargestPropertyUseType',\n",
       "       'ENERGYSTARScore', 'SteamUse(kBtu)', 'NaturalGas(kBtu)'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "GHG_rfr_model=rfr_grid_cv.fit(X_train, Y_train[\"TotalGHGEmissions\"])\n",
    "GHG_results=pd.concat([GHG_results, scores(GHG_rfr_model,'grid_search_rfr')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "EUI_rfr_model=rfr_grid_cv.fit(X_train, Y_train['SourceEUI(kBtu/sf)'])\n",
    "EUI_results=pd.concat([EUI_results, scores(EUI_rfr_model,'grid_search_rfr')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "GHG_test_results=pd.concat([GHG_test_results,test_scores('Random Forest',GHG_rfr_model,X_test,Y_test[\"TotalGHGEmissions\"])])\n",
    "EUI_test_results=pd.concat([EUI_test_results,test_scores('Random Forest',EUI_rfr_model,X_test,Y_test['SourceEUI(kBtu/sf)'])])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il s'agit d'un alghorithme qui supperpose des abres de décision , chaque modele sera appliqué sur les residu du modèle precedent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_xgb = {'learning_rate' : [0.001, 0.01, 0.1, 0.2, 0,3],\n",
    "             'gamma': [0, 0.25, 0.5, 1.0],\n",
    "             'max_depth': [6, 10, 15, 20],\n",
    "             'min_child_weight' : [0.5, 1.0, 3.0, 5.0, 7.0, 10.0],\n",
    "             'n_estimators': [25, 50, 100]}\n",
    "\n",
    "xgb_grid_cv = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('grid_search_xgb', GridSearchCV(xgb.XGBRegressor(),\n",
    "                            param_grid=param_xgb,\n",
    "                            cv=5,\n",
    "                            scoring=('r2','neg_mean_absolute_error','neg_mean_squared_error'),\n",
    "                            refit='neg_mean_absolute_error',\n",
    "                            return_train_score = True,\n",
    "                            n_jobs = -1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1764 entries, 1050 to 860\n",
      "Data columns (total 12 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   BuildingType                  1764 non-null   object \n",
      " 1   PrimaryPropertyType           1764 non-null   object \n",
      " 2   CouncilDistrictCode           1764 non-null   int64  \n",
      " 3   Neighborhood                  1764 non-null   object \n",
      " 4   YearBuilt                     1764 non-null   int64  \n",
      " 5   NumberofFloors                1764 non-null   float64\n",
      " 6   PropertyGFATotal              1764 non-null   float64\n",
      " 7   PropertyGFABuilding(s)        1764 non-null   float64\n",
      " 8   SecondLargestPropertyUseType  1764 non-null   object \n",
      " 9   ENERGYSTARScore               1764 non-null   float64\n",
      " 10  SteamUse(kBtu)                1764 non-null   int64  \n",
      " 11  NaturalGas(kBtu)              1764 non-null   int64  \n",
      "dtypes: float64(4), int64(4), object(4)\n",
      "memory usage: 179.2+ KB\n"
     ]
    }
   ],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "GHG_xgb_model=xgb_grid_cv.fit(X_train, Y_train[\"TotalGHGEmissions\"])\n",
    "GHG_results=pd.concat([GHG_results, scores(GHG_xgb_model,'grid_search_xgb')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "EUI_xgb_model=xgb_grid_cv.fit(X_train, Y_train['SourceEUI(kBtu/sf)'])\n",
    "EUI_results=pd.concat([EUI_results, scores(EUI_xgb_model,'grid_search_xgb')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "GHG_test_results=pd.concat([GHG_test_results,test_scores('XGBoost',GHG_xgb_model,X_test,Y_test[\"TotalGHGEmissions\"])])\n",
    "EUI_test_results=pd.concat([EUI_test_results,test_scores('XGBoost',EUI_xgb_model,X_test,Y_test['SourceEUI(kBtu/sf)'])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ajoute le nom des alghorithme utilisé et les place en premiere colonne du DataFrame\n",
    "name=['Regression linéaire','ElasticNet','Support Vector','Random Forest','XGBoost']\n",
    "GHG_results['name']=name\n",
    "cols=GHG_results.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "GHG_results=GHG_results[cols]\n",
    "\n",
    "EUI_results['name']=name\n",
    "cols=EUI_results.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "EUI_results=EUI_results[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_r2</th>\n",
       "      <th>split1_test_r2</th>\n",
       "      <th>split2_test_r2</th>\n",
       "      <th>split3_test_r2</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_neg_mean_squared_error</th>\n",
       "      <th>std_test_neg_mean_squared_error</th>\n",
       "      <th>rank_test_neg_mean_squared_error</th>\n",
       "      <th>split0_train_neg_mean_squared_error</th>\n",
       "      <th>split1_train_neg_mean_squared_error</th>\n",
       "      <th>split2_train_neg_mean_squared_error</th>\n",
       "      <th>split3_train_neg_mean_squared_error</th>\n",
       "      <th>split4_train_neg_mean_squared_error</th>\n",
       "      <th>mean_train_neg_mean_squared_error</th>\n",
       "      <th>std_train_neg_mean_squared_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Regression linéaire</td>\n",
       "      <td>0.006801</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.001201</td>\n",
       "      <td>4.005194e-04</td>\n",
       "      <td>{'fit_intercept': True}</td>\n",
       "      <td>0.610725</td>\n",
       "      <td>0.632993</td>\n",
       "      <td>0.508543</td>\n",
       "      <td>0.563661</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.796764</td>\n",
       "      <td>0.060304</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.781997</td>\n",
       "      <td>-0.792184</td>\n",
       "      <td>-0.751358</td>\n",
       "      <td>-0.771171</td>\n",
       "      <td>-0.763948</td>\n",
       "      <td>-0.772132</td>\n",
       "      <td>0.014134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>ElasticNet</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>3.162980e-07</td>\n",
       "      <td>{'alpha': 0.01, 'l1_ratio': 0.2, 'max_iter': 10}</td>\n",
       "      <td>0.609197</td>\n",
       "      <td>0.630209</td>\n",
       "      <td>0.509467</td>\n",
       "      <td>0.556276</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.801540</td>\n",
       "      <td>0.058381</td>\n",
       "      <td>91</td>\n",
       "      <td>-0.785689</td>\n",
       "      <td>-0.796043</td>\n",
       "      <td>-0.755287</td>\n",
       "      <td>-0.774685</td>\n",
       "      <td>-0.767863</td>\n",
       "      <td>-0.775913</td>\n",
       "      <td>0.014089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>Support Vector</td>\n",
       "      <td>0.089220</td>\n",
       "      <td>0.007680</td>\n",
       "      <td>0.001001</td>\n",
       "      <td>3.234067e-07</td>\n",
       "      <td>{'C': 0.1, 'epsilon': 0, 'loss': 'squared_epsi...</td>\n",
       "      <td>0.613054</td>\n",
       "      <td>0.625338</td>\n",
       "      <td>0.508194</td>\n",
       "      <td>0.561702</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.801711</td>\n",
       "      <td>0.058311</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.785084</td>\n",
       "      <td>-0.795427</td>\n",
       "      <td>-0.755179</td>\n",
       "      <td>-0.774191</td>\n",
       "      <td>-0.766426</td>\n",
       "      <td>-0.775261</td>\n",
       "      <td>0.014040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.347478</td>\n",
       "      <td>0.005239</td>\n",
       "      <td>0.017604</td>\n",
       "      <td>2.245380e-03</td>\n",
       "      <td>{'bootstrap': False, 'max_depth': 25, 'max_fea...</td>\n",
       "      <td>0.874216</td>\n",
       "      <td>0.896895</td>\n",
       "      <td>0.846838</td>\n",
       "      <td>0.858274</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.255991</td>\n",
       "      <td>0.033024</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.000386</td>\n",
       "      <td>-0.000459</td>\n",
       "      <td>-0.000354</td>\n",
       "      <td>-0.000380</td>\n",
       "      <td>-0.000346</td>\n",
       "      <td>-0.000385</td>\n",
       "      <td>0.000040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>1.687780</td>\n",
       "      <td>0.059426</td>\n",
       "      <td>0.004401</td>\n",
       "      <td>4.899599e-04</td>\n",
       "      <td>{'gamma': 0, 'learning_rate': 0.1, 'max_depth'...</td>\n",
       "      <td>0.855878</td>\n",
       "      <td>0.894104</td>\n",
       "      <td>0.842438</td>\n",
       "      <td>0.854767</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.268642</td>\n",
       "      <td>0.033905</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.001188</td>\n",
       "      <td>-0.001265</td>\n",
       "      <td>-0.000968</td>\n",
       "      <td>-0.001104</td>\n",
       "      <td>-0.000903</td>\n",
       "      <td>-0.001086</td>\n",
       "      <td>0.000134</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    name  mean_fit_time  std_fit_time  mean_score_time  \\\n",
       "0    Regression linéaire       0.006801      0.000400         0.001201   \n",
       "72            ElasticNet       0.000800      0.000400         0.001000   \n",
       "113       Support Vector       0.089220      0.007680         0.001001   \n",
       "144        Random Forest       0.347478      0.005239         0.017604   \n",
       "206              XGBoost       1.687780      0.059426         0.004401   \n",
       "\n",
       "     std_score_time                                             params  \\\n",
       "0      4.005194e-04                            {'fit_intercept': True}   \n",
       "72     3.162980e-07   {'alpha': 0.01, 'l1_ratio': 0.2, 'max_iter': 10}   \n",
       "113    3.234067e-07  {'C': 0.1, 'epsilon': 0, 'loss': 'squared_epsi...   \n",
       "144    2.245380e-03  {'bootstrap': False, 'max_depth': 25, 'max_fea...   \n",
       "206    4.899599e-04  {'gamma': 0, 'learning_rate': 0.1, 'max_depth'...   \n",
       "\n",
       "     split0_test_r2  split1_test_r2  split2_test_r2  split3_test_r2  ...  \\\n",
       "0          0.610725        0.632993        0.508543        0.563661  ...   \n",
       "72         0.609197        0.630209        0.509467        0.556276  ...   \n",
       "113        0.613054        0.625338        0.508194        0.561702  ...   \n",
       "144        0.874216        0.896895        0.846838        0.858274  ...   \n",
       "206        0.855878        0.894104        0.842438        0.854767  ...   \n",
       "\n",
       "     mean_test_neg_mean_squared_error  std_test_neg_mean_squared_error  \\\n",
       "0                           -0.796764                         0.060304   \n",
       "72                          -0.801540                         0.058381   \n",
       "113                         -0.801711                         0.058311   \n",
       "144                         -0.255991                         0.033024   \n",
       "206                         -0.268642                         0.033905   \n",
       "\n",
       "     rank_test_neg_mean_squared_error  split0_train_neg_mean_squared_error  \\\n",
       "0                                   1                            -0.781997   \n",
       "72                                 91                            -0.785689   \n",
       "113                                 2                            -0.785084   \n",
       "144                                 1                            -0.000386   \n",
       "206                                10                            -0.001188   \n",
       "\n",
       "     split1_train_neg_mean_squared_error  split2_train_neg_mean_squared_error  \\\n",
       "0                              -0.792184                            -0.751358   \n",
       "72                             -0.796043                            -0.755287   \n",
       "113                            -0.795427                            -0.755179   \n",
       "144                            -0.000459                            -0.000354   \n",
       "206                            -0.001265                            -0.000968   \n",
       "\n",
       "     split3_train_neg_mean_squared_error  split4_train_neg_mean_squared_error  \\\n",
       "0                              -0.771171                            -0.763948   \n",
       "72                             -0.774685                            -0.767863   \n",
       "113                            -0.774191                            -0.766426   \n",
       "144                            -0.000380                            -0.000346   \n",
       "206                            -0.001104                            -0.000903   \n",
       "\n",
       "     mean_train_neg_mean_squared_error  std_train_neg_mean_squared_error  \n",
       "0                            -0.772132                          0.014134  \n",
       "72                           -0.775913                          0.014089  \n",
       "113                          -0.775261                          0.014040  \n",
       "144                          -0.000385                          0.000040  \n",
       "206                          -0.001086                          0.000134  \n",
       "\n",
       "[5 rows x 51 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['name', 'mean_fit_time', 'std_fit_time', 'mean_score_time',\n",
       "       'std_score_time', 'params', 'split0_test_r2', 'split1_test_r2',\n",
       "       'split2_test_r2', 'split3_test_r2', 'split4_test_r2', 'mean_test_r2',\n",
       "       'std_test_r2', 'rank_test_r2', 'split0_train_r2', 'split1_train_r2',\n",
       "       'split2_train_r2', 'split3_train_r2', 'split4_train_r2',\n",
       "       'mean_train_r2', 'std_train_r2', 'split0_test_neg_mean_absolute_error',\n",
       "       'split1_test_neg_mean_absolute_error',\n",
       "       'split2_test_neg_mean_absolute_error',\n",
       "       'split3_test_neg_mean_absolute_error',\n",
       "       'split4_test_neg_mean_absolute_error',\n",
       "       'mean_test_neg_mean_absolute_error', 'std_test_neg_mean_absolute_error',\n",
       "       'rank_test_neg_mean_absolute_error',\n",
       "       'split0_train_neg_mean_absolute_error',\n",
       "       'split1_train_neg_mean_absolute_error',\n",
       "       'split2_train_neg_mean_absolute_error',\n",
       "       'split3_train_neg_mean_absolute_error',\n",
       "       'split4_train_neg_mean_absolute_error',\n",
       "       'mean_train_neg_mean_absolute_error',\n",
       "       'std_train_neg_mean_absolute_error',\n",
       "       'split0_test_neg_mean_squared_error',\n",
       "       'split1_test_neg_mean_squared_error',\n",
       "       'split2_test_neg_mean_squared_error',\n",
       "       'split3_test_neg_mean_squared_error',\n",
       "       'split4_test_neg_mean_squared_error',\n",
       "       'mean_test_neg_mean_squared_error', 'std_test_neg_mean_squared_error',\n",
       "       'rank_test_neg_mean_squared_error',\n",
       "       'split0_train_neg_mean_squared_error',\n",
       "       'split1_train_neg_mean_squared_error',\n",
       "       'split2_train_neg_mean_squared_error',\n",
       "       'split3_train_neg_mean_squared_error',\n",
       "       'split4_train_neg_mean_squared_error',\n",
       "       'mean_train_neg_mean_squared_error',\n",
       "       'std_train_neg_mean_squared_error'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EUI_results.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>test_r2</th>\n",
       "      <th>test_median_abs_error</th>\n",
       "      <th>test-mean-absolute_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Regression linéaire</td>\n",
       "      <td>-0.086697</td>\n",
       "      <td>1.011104</td>\n",
       "      <td>1.200885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ElasticNet</td>\n",
       "      <td>-0.10836</td>\n",
       "      <td>1.049903</td>\n",
       "      <td>1.210226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Support Vector</td>\n",
       "      <td>-0.232029</td>\n",
       "      <td>1.087944</td>\n",
       "      <td>1.273606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.011811</td>\n",
       "      <td>1.034282</td>\n",
       "      <td>1.159415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.018005</td>\n",
       "      <td>1.012292</td>\n",
       "      <td>1.151558</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  name   test_r2 test_median_abs_error  \\\n",
       "0  Regression linéaire -0.086697              1.011104   \n",
       "0           ElasticNet  -0.10836              1.049903   \n",
       "0       Support Vector -0.232029              1.087944   \n",
       "0        Random Forest  0.011811              1.034282   \n",
       "0              XGBoost  0.018005              1.012292   \n",
       "\n",
       "  test-mean-absolute_error  \n",
       "0                 1.200885  \n",
       "0                 1.210226  \n",
       "0                 1.273606  \n",
       "0                 1.159415  \n",
       "0                 1.151558  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GHG_test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>test_r2</th>\n",
       "      <th>test_median_abs_error</th>\n",
       "      <th>test-mean-absolute_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Regression linéaire</td>\n",
       "      <td>0.470293</td>\n",
       "      <td>0.283354</td>\n",
       "      <td>0.414825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ElasticNet</td>\n",
       "      <td>0.465835</td>\n",
       "      <td>0.269836</td>\n",
       "      <td>0.41361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Support Vector</td>\n",
       "      <td>0.415677</td>\n",
       "      <td>0.304661</td>\n",
       "      <td>0.433685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.890295</td>\n",
       "      <td>0.06888</td>\n",
       "      <td>0.142776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.892547</td>\n",
       "      <td>0.069762</td>\n",
       "      <td>0.146231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  name   test_r2 test_median_abs_error  \\\n",
       "0  Regression linéaire  0.470293              0.283354   \n",
       "0           ElasticNet  0.465835              0.269836   \n",
       "0       Support Vector  0.415677              0.304661   \n",
       "0        Random Forest  0.890295               0.06888   \n",
       "0              XGBoost  0.892547              0.069762   \n",
       "\n",
       "  test-mean-absolute_error  \n",
       "0                 0.414825  \n",
       "0                  0.41361  \n",
       "0                 0.433685  \n",
       "0                 0.142776  \n",
       "0                 0.146231  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EUI_test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "noES_EUI_test_results['ESS']=False\n",
    "noES_GHG_test_results['ESS']=False\n",
    "noES_GHG_results['ESS']=False\n",
    "noES_EUI_results['ESS']=False\n",
    "\n",
    "\n",
    "EUI_test_results['ESS']=True\n",
    "GHG_test_results['ESS']=True\n",
    "GHG_results['ESS']=True\n",
    "EUI_results['ESS']=True\n",
    "\n",
    "EUI_test_results=pd.concat([EUI_test_results,noES_EUI_test_results])\n",
    "EUI_results=pd.concat([EUI_results,noES_EUI_results])\n",
    "GHG_test_results=pd.concat([GHG_test_results,noES_GHG_test_results])\n",
    "GHG_results=pd.concat([GHG_results,noES_GHG_results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>test_r2</th>\n",
       "      <th>test_median_abs_error</th>\n",
       "      <th>test-mean-absolute_error</th>\n",
       "      <th>ESS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Regression linéaire</td>\n",
       "      <td>0.470293</td>\n",
       "      <td>0.283354</td>\n",
       "      <td>0.414825</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ElasticNet</td>\n",
       "      <td>0.465835</td>\n",
       "      <td>0.269836</td>\n",
       "      <td>0.41361</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Support Vector</td>\n",
       "      <td>0.415677</td>\n",
       "      <td>0.304661</td>\n",
       "      <td>0.433685</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.890295</td>\n",
       "      <td>0.06888</td>\n",
       "      <td>0.142776</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.892547</td>\n",
       "      <td>0.069762</td>\n",
       "      <td>0.146231</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Regression linéaire</td>\n",
       "      <td>0.173529</td>\n",
       "      <td>0.396921</td>\n",
       "      <td>0.531787</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ElasticNet</td>\n",
       "      <td>0.172396</td>\n",
       "      <td>0.372902</td>\n",
       "      <td>0.530865</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Support Vector</td>\n",
       "      <td>0.152741</td>\n",
       "      <td>0.410651</td>\n",
       "      <td>0.539831</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.745334</td>\n",
       "      <td>0.079784</td>\n",
       "      <td>0.205095</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.702977</td>\n",
       "      <td>0.181377</td>\n",
       "      <td>0.281247</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  name   test_r2 test_median_abs_error  \\\n",
       "0  Regression linéaire  0.470293              0.283354   \n",
       "0           ElasticNet  0.465835              0.269836   \n",
       "0       Support Vector  0.415677              0.304661   \n",
       "0        Random Forest  0.890295               0.06888   \n",
       "0              XGBoost  0.892547              0.069762   \n",
       "0  Regression linéaire  0.173529              0.396921   \n",
       "1           ElasticNet  0.172396              0.372902   \n",
       "2       Support Vector  0.152741              0.410651   \n",
       "3        Random Forest  0.745334              0.079784   \n",
       "4              XGBoost  0.702977              0.181377   \n",
       "\n",
       "  test-mean-absolute_error    ESS  \n",
       "0                 0.414825   True  \n",
       "0                  0.41361   True  \n",
       "0                 0.433685   True  \n",
       "0                 0.142776   True  \n",
       "0                 0.146231   True  \n",
       "0                 0.531787  False  \n",
       "1                 0.530865  False  \n",
       "2                 0.539831  False  \n",
       "3                 0.205095  False  \n",
       "4                 0.281247  False  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EUI_test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction GHG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant comparer les résultat obtenue pour la prediction de la variable GHG afin de choisir le meilleur modele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='name'>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1,2)\n",
    "GHG_results[GHG_results['ESS']==True][['name','mean_fit_time','std_fit_time']].plot(x='name',kind='bar',figsize=(10,8),ax=axs[0])\n",
    "GHG_results[GHG_results['ESS']==False][['name','mean_fit_time','std_fit_time']].plot(x='name',kind='bar',figsize=(10,8),ax=axs[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le temps d'entraiment des modèles linéaire est superieur au celui des modèles linéaires cependant le modele xgboost est aussi beaucoup plus lent a entrainé que le RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x1008 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "fig, axs = plt.subplots(3,2,figsize=(16,14))\n",
    "\n",
    "box= GHG_results[GHG_results['ESS']==True].set_index('name').copy()\n",
    "box[['split0_test_r2','split1_test_r2','split2_test_r2','split3_test_r2','split4_test_r2']].T.boxplot(ax=axs[0,0],whis=100)\n",
    "box= GHG_results[GHG_results['ESS']==False].set_index('name').copy()\n",
    "box[['split0_test_r2','split1_test_r2','split2_test_r2','split3_test_r2','split4_test_r2']].T.boxplot(ax=axs[0,1],whis=100)\n",
    "\n",
    "\n",
    "\n",
    "box= GHG_results[GHG_results['ESS']==True].set_index('name').copy()\n",
    "box[['split0_test_neg_mean_absolute_error','split1_test_neg_mean_absolute_error','split2_test_neg_mean_absolute_error','split3_test_neg_mean_absolute_error','split4_test_neg_mean_absolute_error']].T.boxplot(ax=axs[1,0],whis=1000)\n",
    "box= GHG_results[GHG_results['ESS']==False].set_index('name').copy()\n",
    "box[['split0_test_neg_mean_absolute_error','split1_test_neg_mean_absolute_error','split2_test_neg_mean_absolute_error','split3_test_neg_mean_absolute_error','split4_test_neg_mean_absolute_error']].T.boxplot(ax=axs[1,1],whis=1000)\n",
    "\n",
    "box= GHG_results[GHG_results['ESS']==True].set_index('name').copy()\n",
    "box[['split0_test_neg_mean_squared_error','split1_test_neg_mean_squared_error','split2_test_neg_mean_squared_error','split3_test_neg_mean_squared_error','split4_test_neg_mean_squared_error']].T.boxplot(ax=axs[2,0],whis=1000)\n",
    "box= GHG_results[GHG_results['ESS']==False].set_index('name').copy()\n",
    "box[['split0_test_neg_mean_squared_error','split1_test_neg_mean_squared_error','split2_test_neg_mean_squared_error','split3_test_neg_mean_squared_error','split4_test_neg_mean_squared_error']].T.boxplot(ax=axs[2,1],whis=1000)\n",
    "\n",
    "axs[0, 0].set_title('R2 avec ESS')\n",
    "axs[0, 1].set_title('R2 sans ESS')\n",
    "axs[1, 0].set_title('neg_mean_absolute_error avec ESS')\n",
    "axs[1, 1].set_title('neg_mean_absolute_error sans ESS')\n",
    "axs[2, 0].set_title('neg_mean_squared_error avec ESS')\n",
    "axs[2, 1].set_title('neg_mean_squared_error sans ESS')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les modèles XGboost et RandomForest donne de bien meilleurs résultats pour nos métrique d'erreur.\n",
    "On peut remarquer que la variable ENERGYSTARScore ammeliore legérement les perfomances du modèles XGBoost mais cela ne permet pas d'obtenir des meilleur scores que le modèle RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'neg_mean_squared_error sans ESS')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x1008 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(3,2,figsize=(16,14))\n",
    "\n",
    "box= GHG_results[GHG_results['ESS']==True].set_index('name').copy()\n",
    "box[['split0_train_r2','split1_train_r2','split2_train_r2','split3_train_r2','split4_train_r2']].T.boxplot(ax=axs[0,0],whis=100)\n",
    "box= GHG_results[GHG_results['ESS']==False].set_index('name').copy()\n",
    "box[['split0_train_r2','split1_train_r2','split2_train_r2','split3_train_r2','split4_train_r2']].T.boxplot(ax=axs[0,1],whis=100)\n",
    "\n",
    "\n",
    "\n",
    "box= GHG_results[GHG_results['ESS']==True].set_index('name').copy()\n",
    "box[['split0_train_neg_mean_absolute_error','split1_train_neg_mean_absolute_error','split2_train_neg_mean_absolute_error','split3_train_neg_mean_absolute_error','split4_train_neg_mean_absolute_error']].T.boxplot(ax=axs[1,0],whis=100)\n",
    "box= GHG_results[GHG_results['ESS']==False].set_index('name').copy()\n",
    "box[['split0_train_neg_mean_absolute_error','split1_train_neg_mean_absolute_error','split2_train_neg_mean_absolute_error','split3_train_neg_mean_absolute_error','split4_train_neg_mean_absolute_error']].T.boxplot(ax=axs[1,1],whis=100)\n",
    "\n",
    "box= GHG_results[GHG_results['ESS']==True].set_index('name').copy()\n",
    "box[['split0_train_neg_mean_squared_error','split1_train_neg_mean_squared_error','split2_train_neg_mean_squared_error','split3_train_neg_mean_squared_error','split4_train_neg_mean_squared_error']].T.boxplot(ax=axs[2,0],whis=100)\n",
    "box= GHG_results[GHG_results['ESS']==False].set_index('name').copy()\n",
    "box[['split0_train_neg_mean_squared_error','split1_train_neg_mean_squared_error','split2_train_neg_mean_squared_error','split3_train_neg_mean_squared_error','split4_train_neg_mean_squared_error']].T.boxplot(ax=axs[2,1],whis=100)\n",
    "\n",
    "axs[0, 0].set_title('R2 avec ESS')\n",
    "axs[0, 1].set_title('R2 sans ESS')\n",
    "axs[1, 0].set_title('neg_mean_absolute_error avec ESS')\n",
    "axs[1, 1].set_title('neg_mean_absolute_error sans ESS')\n",
    "axs[2, 0].set_title('neg_mean_squared_error avec ESS')\n",
    "axs[2, 1].set_title('neg_mean_squared_error sans ESS')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les résultats sont très similaires a ceux obtenue sur les jeux de test\n",
    "Les modèles XGboost et RandomForest donne de bien meilleurs résultats pour nos métrique d'erreur.\n",
    "On peut remarquer que la variable ENERGYSTARScore ammeliore legérement les perfomances du modèles XGBoost mais cela ne permet pas d'obtenir des meilleur scores que le modèle RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='name'>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(figsize=(16,8),ncols=2)\n",
    "GHG_test_results[GHG_test_results['ESS']==True].plot(x='name',kind='bar',ax=axs[0])\n",
    "GHG_test_results[GHG_test_results['ESS']==False].plot(x='name',kind='bar',ax=axs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme précedement les modèles XGBoost et RandomForest offrent de meilleur résultats .\n",
    "La presence de lavariable ENERGYSTARScore améliore legerement les valeurs de la métrique R2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En conclusion les modèles RanfomForest et XgBoost on des performances similaires quand à la prédiction de la variable \"TotalGHGEmissions\" , cependant le modèle XGBoost étant beaucoup plus lent à entrainer , le modele RandomForest semble etre le meilleur choix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction EUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'sans ESS')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(figsize=(16,8),ncols=2)\n",
    "\n",
    "EUI_results[EUI_results['ESS']==True][['name','std_fit_time','std_fit_time']].plot(x='name',kind='bar',ax=axs[0])\n",
    "axs[0].set_title('avec ESS')\n",
    "\n",
    "EUI_results[EUI_results['ESS']==False][['name','mean_fit_time','std_fit_time']].plot(x='name',kind='bar',ax=axs[1])\n",
    "axs[1].set_title('sans ESS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x1008 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "fig, axs = plt.subplots(3,2,figsize=(16,14))\n",
    "\n",
    "\n",
    "box= EUI_results[EUI_results['ESS']==True].set_index('name').copy()\n",
    "box[['split0_test_r2','split1_test_r2','split2_test_r2','split3_test_r2','split4_test_r2']].T.boxplot(ax=axs[0,0],whis=100)\n",
    "box= EUI_results[EUI_results['ESS']==False].set_index('name').copy()\n",
    "box[['split0_test_r2','split1_test_r2','split2_test_r2','split3_test_r2','split4_test_r2']].T.boxplot(ax=axs[0,1],whis=100)\n",
    "    \n",
    "box= EUI_results[EUI_results['ESS']==True].set_index('name').copy()\n",
    "box[['split0_test_neg_mean_absolute_error','split1_test_neg_mean_absolute_error','split2_test_neg_mean_absolute_error','split3_test_neg_mean_absolute_error','split4_test_neg_mean_absolute_error']].T.boxplot(ax=axs[1,0],whis=100)\n",
    "box= EUI_results[EUI_results['ESS']==False].set_index('name').copy()\n",
    "box[['split0_test_neg_mean_absolute_error','split1_test_neg_mean_absolute_error','split2_test_neg_mean_absolute_error','split3_test_neg_mean_absolute_error','split4_test_neg_mean_absolute_error']].T.boxplot(ax=axs[1,1],whis=100)\n",
    "\n",
    "box= EUI_results[EUI_results['ESS']==True].set_index('name').copy()\n",
    "box[['split0_test_neg_mean_squared_error','split1_test_neg_mean_squared_error','split2_test_neg_mean_squared_error','split3_test_neg_mean_squared_error','split4_test_neg_mean_squared_error']].T.boxplot(ax=axs[2,0],whis=100)\n",
    "box= EUI_results[EUI_results['ESS']==False].set_index('name').copy()\n",
    "box[['split0_test_neg_mean_squared_error','split1_test_neg_mean_squared_error','split2_test_neg_mean_squared_error','split3_test_neg_mean_squared_error','split4_test_neg_mean_squared_error']].T.boxplot(ax=axs[2,1],whis=100)\n",
    "\n",
    "axs[0, 0].set_title('R2 avec ESS')\n",
    "axs[0, 1].set_title('R2 sans ESS')\n",
    "axs[1, 0].set_title('neg_mean_absolute_error avec ESS')\n",
    "axs[1, 1].set_title('neg_mean_absolute_error sans ESS')\n",
    "axs[2, 0].set_title('neg_mean_squared_error avec ESS')\n",
    "axs[2, 1].set_title('neg_mean_squared_error sans ESS')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'neg_mean_squared_error sans ESS')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x1008 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(3,2,figsize=(16,14))\n",
    "\n",
    "box= EUI_results[EUI_results['ESS']==True].set_index('name').copy()\n",
    "box[['split0_train_r2','split1_train_r2','split2_train_r2','split3_train_r2','split4_train_r2']].T.boxplot(ax=axs[0,0],whis=100)\n",
    "box= EUI_results[EUI_results['ESS']==False].set_index('name').copy()\n",
    "box[['split0_train_r2','split1_train_r2','split2_train_r2','split3_train_r2','split4_train_r2']].T.boxplot(ax=axs[0,1],whis=100)\n",
    "\n",
    "\n",
    "\n",
    "box= EUI_results[EUI_results['ESS']==True].set_index('name').copy()\n",
    "box[['split0_train_neg_mean_absolute_error','split1_train_neg_mean_absolute_error','split2_train_neg_mean_absolute_error','split3_train_neg_mean_absolute_error','split4_train_neg_mean_absolute_error']].T.boxplot(ax=axs[1,0],whis=100)\n",
    "box= EUI_results[EUI_results['ESS']==False].set_index('name').copy()\n",
    "box[['split0_train_neg_mean_absolute_error','split1_train_neg_mean_absolute_error','split2_train_neg_mean_absolute_error','split3_train_neg_mean_absolute_error','split4_train_neg_mean_absolute_error']].T.boxplot(ax=axs[1,1],whis=100)\n",
    "\n",
    "box= EUI_results[EUI_results['ESS']==True].set_index('name').copy()\n",
    "box[['split0_train_neg_mean_squared_error','split1_train_neg_mean_squared_error','split2_train_neg_mean_squared_error','split3_train_neg_mean_squared_error','split4_train_neg_mean_squared_error']].T.boxplot(ax=axs[2,0],whis=100)\n",
    "box= EUI_results[EUI_results['ESS']==False].set_index('name').copy()\n",
    "box[['split0_train_neg_mean_squared_error','split1_train_neg_mean_squared_error','split2_train_neg_mean_squared_error','split3_train_neg_mean_squared_error','split4_train_neg_mean_squared_error']].T.boxplot(ax=axs[2,1],whis=100)\n",
    "\n",
    "axs[0, 0].set_title('R2 avec ESS')\n",
    "axs[0, 1].set_title('R2 sans ESS')\n",
    "axs[1, 0].set_title('neg_mean_absolute_error avec ESS')\n",
    "axs[1, 1].set_title('neg_mean_absolute_error sans ESS')\n",
    "axs[2, 0].set_title('neg_mean_squared_error avec ESS')\n",
    "axs[2, 1].set_title('neg_mean_squared_error sans ESS')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='name'>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(figsize=(16,8),ncols=2)\n",
    "EUI_test_results[EUI_test_results['ESS']==True].plot(x='name',kind='bar',ax=axs[0])\n",
    "EUI_test_results[EUI_test_results['ESS']==False].plot(x='name',kind='bar',ax=axs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec la variable ENERGYSTARScore on tombe a des résultats qui sont plus précis que ceux obtenue lors de la prédiction de la variable \"TotalGHGEmissions\"\n",
    "De plus comme précedement le deux modèles XGBoost et RandomForest on des résultats très similaires cependant le modèle de forêt aléatoire étant bien moin long à entrainer, nous le prefererons aux XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BuildingType</th>\n",
       "      <th>PrimaryPropertyType</th>\n",
       "      <th>CouncilDistrictCode</th>\n",
       "      <th>Neighborhood</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>NumberofFloors</th>\n",
       "      <th>PropertyGFATotal</th>\n",
       "      <th>PropertyGFABuilding(s)</th>\n",
       "      <th>SecondLargestPropertyUseType</th>\n",
       "      <th>ENERGYSTARScore</th>\n",
       "      <th>SteamUse(kBtu)</th>\n",
       "      <th>NaturalGas(kBtu)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1050</th>\n",
       "      <td>NONRESIDENTIAL</td>\n",
       "      <td>SMALL- AND MID-SIZED OFFICE</td>\n",
       "      <td>3</td>\n",
       "      <td>EAST</td>\n",
       "      <td>1920</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7807.333333</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>SMALL- AND MID-SIZED OFFICE</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1967</th>\n",
       "      <td>NONRESIDENTIAL</td>\n",
       "      <td>WAREHOUSE</td>\n",
       "      <td>2</td>\n",
       "      <td>GREATER DUWAMISH</td>\n",
       "      <td>1926</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10200.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>OFFICE</td>\n",
       "      <td>69.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1387</th>\n",
       "      <td>NONRESIDENTIAL</td>\n",
       "      <td>LARGE OFFICE</td>\n",
       "      <td>2</td>\n",
       "      <td>DOWNTOWN</td>\n",
       "      <td>2002</td>\n",
       "      <td>10.0</td>\n",
       "      <td>18169.400000</td>\n",
       "      <td>78.983346</td>\n",
       "      <td>PARKING</td>\n",
       "      <td>88.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1639</th>\n",
       "      <td>NONRESIDENTIAL</td>\n",
       "      <td>HOTEL</td>\n",
       "      <td>4</td>\n",
       "      <td>NORTHEAST</td>\n",
       "      <td>1930</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6857.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>HOTEL</td>\n",
       "      <td>72.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1652</th>\n",
       "      <td>NONRESIDENTIAL</td>\n",
       "      <td>K-12 SCHOOL</td>\n",
       "      <td>1</td>\n",
       "      <td>SOUTHWEST</td>\n",
       "      <td>1922</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16731.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>K-12 SCHOOL</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1638</th>\n",
       "      <td>NONRESIDENTIAL</td>\n",
       "      <td>RETAIL STORE</td>\n",
       "      <td>4</td>\n",
       "      <td>NORTHEAST</td>\n",
       "      <td>1900</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10986.666667</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>RETAIL STORE</td>\n",
       "      <td>71.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1095</th>\n",
       "      <td>NONRESIDENTIAL</td>\n",
       "      <td>MIXED USE PROPERTY</td>\n",
       "      <td>7</td>\n",
       "      <td>DOWNTOWN</td>\n",
       "      <td>1902</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12368.500000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>MULTIFAMILY HOUSING</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1130</th>\n",
       "      <td>SPS-DISTRICT K-12</td>\n",
       "      <td>K-12 SCHOOL</td>\n",
       "      <td>2</td>\n",
       "      <td>SOUTHEAST</td>\n",
       "      <td>1961</td>\n",
       "      <td>2.0</td>\n",
       "      <td>28114.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>K-12 SCHOOL</td>\n",
       "      <td>95.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1294</th>\n",
       "      <td>NONRESIDENTIAL</td>\n",
       "      <td>LARGE OFFICE</td>\n",
       "      <td>7</td>\n",
       "      <td>DOWNTOWN</td>\n",
       "      <td>1964</td>\n",
       "      <td>22.0</td>\n",
       "      <td>15287.727273</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>PARKING</td>\n",
       "      <td>77.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>NONRESIDENTIAL</td>\n",
       "      <td>WAREHOUSE</td>\n",
       "      <td>2</td>\n",
       "      <td>GREATER DUWAMISH</td>\n",
       "      <td>1916</td>\n",
       "      <td>3.0</td>\n",
       "      <td>23761.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>OFFICE</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1764 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           BuildingType          PrimaryPropertyType  CouncilDistrictCode  \\\n",
       "1050     NONRESIDENTIAL  SMALL- AND MID-SIZED OFFICE                    3   \n",
       "1967     NONRESIDENTIAL                    WAREHOUSE                    2   \n",
       "1387     NONRESIDENTIAL                 LARGE OFFICE                    2   \n",
       "1639     NONRESIDENTIAL                        HOTEL                    4   \n",
       "1652     NONRESIDENTIAL                  K-12 SCHOOL                    1   \n",
       "...                 ...                          ...                  ...   \n",
       "1638     NONRESIDENTIAL                 RETAIL STORE                    4   \n",
       "1095     NONRESIDENTIAL           MIXED USE PROPERTY                    7   \n",
       "1130  SPS-DISTRICT K-12                  K-12 SCHOOL                    2   \n",
       "1294     NONRESIDENTIAL                 LARGE OFFICE                    7   \n",
       "860      NONRESIDENTIAL                    WAREHOUSE                    2   \n",
       "\n",
       "          Neighborhood  YearBuilt  NumberofFloors  PropertyGFATotal  \\\n",
       "1050              EAST       1920             3.0       7807.333333   \n",
       "1967  GREATER DUWAMISH       1926             2.0      10200.000000   \n",
       "1387          DOWNTOWN       2002            10.0      18169.400000   \n",
       "1639         NORTHEAST       1930             3.0       6857.000000   \n",
       "1652         SOUTHWEST       1922             2.0      16731.000000   \n",
       "...                ...        ...             ...               ...   \n",
       "1638         NORTHEAST       1900             3.0      10986.666667   \n",
       "1095          DOWNTOWN       1902             6.0      12368.500000   \n",
       "1130         SOUTHEAST       1961             2.0      28114.000000   \n",
       "1294          DOWNTOWN       1964            22.0      15287.727273   \n",
       "860   GREATER DUWAMISH       1916             3.0      23761.000000   \n",
       "\n",
       "      PropertyGFABuilding(s) SecondLargestPropertyUseType  ENERGYSTARScore  \\\n",
       "1050              100.000000  SMALL- AND MID-SIZED OFFICE             64.0   \n",
       "1967              100.000000                       OFFICE             69.0   \n",
       "1387               78.983346                      PARKING             88.0   \n",
       "1639              100.000000                        HOTEL             72.0   \n",
       "1652              100.000000                  K-12 SCHOOL             76.0   \n",
       "...                      ...                          ...              ...   \n",
       "1638              100.000000                 RETAIL STORE             71.0   \n",
       "1095              100.000000          MULTIFAMILY HOUSING             97.0   \n",
       "1130              100.000000                  K-12 SCHOOL             95.0   \n",
       "1294              100.000000                      PARKING             77.0   \n",
       "860               100.000000                       OFFICE             44.0   \n",
       "\n",
       "      SteamUse(kBtu)  NaturalGas(kBtu)  \n",
       "1050               0                 1  \n",
       "1967               0                 1  \n",
       "1387               0                 0  \n",
       "1639               0                 0  \n",
       "1652               0                 1  \n",
       "...              ...               ...  \n",
       "1638               0                 0  \n",
       "1095               0                 0  \n",
       "1130               0                 1  \n",
       "1294               1                 1  \n",
       "860                0                 1  \n",
       "\n",
       "[1764 rows x 12 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etude meilleur modele GHG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons vu que notre meilleur modèle pour la prédiction de cette variable est le RandomForest, nous allons maintenant observer en detail ce modèle et regarder :\n",
    "    - L'évolution des scores en fonction des hyperparametre\n",
    "    - Le Feature importance du modèle\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "font_title = {'family': 'serif',\n",
    "              'color':  '#1d479b',\n",
    "              'weight': 'bold',\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fonction d'affichage des scores de GridSearch pour chacun des paramètres\n",
    "def plot_search_results(grid, title): \n",
    "       \n",
    "    ## Résultats de la GridSearch\n",
    "    results = grid.cv_results_\n",
    "    means_test = results['mean_test_neg_mean_absolute_error']\n",
    "    stds_test = results['std_test_neg_mean_absolute_error']\n",
    "    means_train = results['mean_train_neg_mean_absolute_error']\n",
    "    stds_train = results['std_train_neg_mean_absolute_error']\n",
    "    \n",
    "\n",
    "    ## Index de valeurs par hyper-paramètre\n",
    "    masks=[]\n",
    "    masks_names= list(grid.best_params_.keys())\n",
    "    for p_k, p_v in grid.best_params_.items():\n",
    "        masks.append(list(results['param_'+p_k].data==p_v))\n",
    "\n",
    "    params=grid.param_grid\n",
    "\n",
    "    \n",
    "    ## Plot des résultats\n",
    "    fig, ax = plt.subplots(1,len(params),sharex='none', sharey='all',figsize=(20,5))\n",
    "    fig.suptitle('Scores par paramètres pour la variable {}'.format(title), \n",
    "                 fontdict=font_title, fontsize=22)\n",
    "    fig.text(0.04, 0.5, 'NEG MEAN ABSOLUTE ERROR SCORE', va='center', rotation='vertical')\n",
    "    pram_preformace_in_best = {}\n",
    "    for i, p in enumerate(masks_names):\n",
    "        m = np.stack(masks[:i] + masks[i+1:])\n",
    "        pram_preformace_in_best\n",
    "        best_parms_mask = m.all(axis=0)\n",
    "        best_index = np.where(best_parms_mask)[0]\n",
    "        x = np.array(params[p])\n",
    "        y_1 = np.array(means_test[best_index])\n",
    "        e_1 = np.array(stds_test[best_index])\n",
    "        y_2 = np.array(means_train[best_index])\n",
    "        e_2 = np.array(stds_train[best_index])\n",
    "        ax[i].errorbar(x, y_1, e_1, linestyle='--', marker='o', label='test', color=\"#2cb7b0\")\n",
    "        ax[i].errorbar(x, y_2, e_2, linestyle='--', marker='o', label='train', color=\"#337da4\")\n",
    "        ax[i].set_xlabel(p.upper())\n",
    "\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nRappel des meilleurs paramètres :\\n{}\".format(grid.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['BuildingType', 'PrimaryPropertyType', 'CouncilDistrictCode',\n",
       "       'Neighborhood', 'YearBuilt', 'NumberofFloors', 'PropertyGFATotal',\n",
       "       'PropertyGFABuilding(s)', 'SecondLargestPropertyUseType',\n",
       "       'ENERGYSTARScore', 'SteamUse(kBtu)', 'NaturalGas(kBtu)'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rappel des meilleurs paramètres :\n",
      "{'bootstrap': False, 'max_depth': 25, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 2}\n"
     ]
    }
   ],
   "source": [
    "plot_search_results(GHG_rfr_model.named_steps['grid_search_rfr'], title=\"TotalGHGEmissions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Récuperation des SHAP values pour le modèle EUI_rfr_model\n",
    "feature_names = X.columns\n",
    "result = permutation_importance(\n",
    "    GHG_rfr_model, X_test, Y_test[\"TotalGHGEmissions\"], n_repeats=10, random_state=42, n_jobs=2\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "forest_importances = pd.Series(result.importances_mean, index=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(16,8))\n",
    "forest_importances.plot.bar(yerr=result.importances_std, ax=ax)\n",
    "ax.set_title(\"Feature importances using permutation on full model\")\n",
    "ax.set_ylabel(\"Mean accuracy decrease\")\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La variable PrimaryPropertyType est de loin la variable la plus importantes pour le modèles , la variable ENERGYSTARScore arrive seulement en 3eme place mble avoir des résultats qui veen sarie en lors des permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "result=X_test.copy()\n",
    "result['result']=GHG_rfr_model.predict(X_test)\n",
    "result['error']=abs(Y_test[\"TotalGHGEmissions\"]-result['result'])\n",
    "sns.displot(x='error',data=result,bins=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "result['y']=Y_test[\"TotalGHGEmissions\"]\n",
    "plt.figure(figsize=(16,8))\n",
    "sns.scatterplot(x='result',y=\"y\",data=result,hue='PrimaryPropertyType',)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut voir que nos données ne sont pas concentré près de la premiere bissectrice, ce qui pose des questions sur la précision du modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BuildingType</th>\n",
       "      <th>PrimaryPropertyType</th>\n",
       "      <th>CouncilDistrictCode</th>\n",
       "      <th>Neighborhood</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>NumberofFloors</th>\n",
       "      <th>PropertyGFATotal</th>\n",
       "      <th>PropertyGFABuilding(s)</th>\n",
       "      <th>SecondLargestPropertyUseType</th>\n",
       "      <th>ENERGYSTARScore</th>\n",
       "      <th>SteamUse(kBtu)</th>\n",
       "      <th>NaturalGas(kBtu)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1966</th>\n",
       "      <td>NONRESIDENTIAL</td>\n",
       "      <td>WAREHOUSE</td>\n",
       "      <td>2</td>\n",
       "      <td>GREATER DUWAMISH</td>\n",
       "      <td>1992</td>\n",
       "      <td>1.0</td>\n",
       "      <td>29394.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>OFFICE</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1091</th>\n",
       "      <td>NONRESIDENTIAL</td>\n",
       "      <td>SUPERMARKET / GROCERY STORE</td>\n",
       "      <td>5</td>\n",
       "      <td>NORTH</td>\n",
       "      <td>2011</td>\n",
       "      <td>1.0</td>\n",
       "      <td>51400.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>PARKING</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1468</th>\n",
       "      <td>NONRESIDENTIAL</td>\n",
       "      <td>MEDICAL OFFICE</td>\n",
       "      <td>3</td>\n",
       "      <td>EAST</td>\n",
       "      <td>1985</td>\n",
       "      <td>10.0</td>\n",
       "      <td>42250.300000</td>\n",
       "      <td>51.105673</td>\n",
       "      <td>MEDICAL OFFICE</td>\n",
       "      <td>46.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>NONRESIDENTIAL</td>\n",
       "      <td>LARGE OFFICE</td>\n",
       "      <td>7</td>\n",
       "      <td>DOWNTOWN</td>\n",
       "      <td>1971</td>\n",
       "      <td>9.0</td>\n",
       "      <td>24984.111111</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>LARGE OFFICE</td>\n",
       "      <td>73.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1790</th>\n",
       "      <td>NONRESIDENTIAL</td>\n",
       "      <td>WORSHIP FACILITY</td>\n",
       "      <td>5</td>\n",
       "      <td>NORTHWEST</td>\n",
       "      <td>1962</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22047.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>PARKING</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1314</th>\n",
       "      <td>NONRESIDENTIAL</td>\n",
       "      <td>HOTEL</td>\n",
       "      <td>7</td>\n",
       "      <td>DOWNTOWN</td>\n",
       "      <td>1973</td>\n",
       "      <td>19.0</td>\n",
       "      <td>17477.210526</td>\n",
       "      <td>82.148181</td>\n",
       "      <td>HOTEL</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>930</th>\n",
       "      <td>NONRESIDENTIAL</td>\n",
       "      <td>SMALL- AND MID-SIZED OFFICE</td>\n",
       "      <td>2</td>\n",
       "      <td>SOUTHEAST</td>\n",
       "      <td>1967</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11625.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>SMALL- AND MID-SIZED OFFICE</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1735</th>\n",
       "      <td>NONRESIDENTIAL</td>\n",
       "      <td>SMALL- AND MID-SIZED OFFICE</td>\n",
       "      <td>7</td>\n",
       "      <td>DOWNTOWN</td>\n",
       "      <td>1907</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5866.666667</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>SMALL- AND MID-SIZED OFFICE</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>NONRESIDENTIAL</td>\n",
       "      <td>SMALL- AND MID-SIZED OFFICE</td>\n",
       "      <td>3</td>\n",
       "      <td>SOUTHEAST</td>\n",
       "      <td>2003</td>\n",
       "      <td>3.0</td>\n",
       "      <td>33144.333333</td>\n",
       "      <td>64.044130</td>\n",
       "      <td>PARKING</td>\n",
       "      <td>71.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>SPS-DISTRICT K-12</td>\n",
       "      <td>K-12 SCHOOL</td>\n",
       "      <td>6</td>\n",
       "      <td>BALLARD</td>\n",
       "      <td>1991</td>\n",
       "      <td>2.0</td>\n",
       "      <td>27756.500000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>K-12 SCHOOL</td>\n",
       "      <td>91.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>441 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           BuildingType          PrimaryPropertyType  CouncilDistrictCode  \\\n",
       "1966     NONRESIDENTIAL                    WAREHOUSE                    2   \n",
       "1091     NONRESIDENTIAL  SUPERMARKET / GROCERY STORE                    5   \n",
       "1468     NONRESIDENTIAL               MEDICAL OFFICE                    3   \n",
       "438      NONRESIDENTIAL                 LARGE OFFICE                    7   \n",
       "1790     NONRESIDENTIAL             WORSHIP FACILITY                    5   \n",
       "...                 ...                          ...                  ...   \n",
       "1314     NONRESIDENTIAL                        HOTEL                    7   \n",
       "930      NONRESIDENTIAL  SMALL- AND MID-SIZED OFFICE                    2   \n",
       "1735     NONRESIDENTIAL  SMALL- AND MID-SIZED OFFICE                    7   \n",
       "59       NONRESIDENTIAL  SMALL- AND MID-SIZED OFFICE                    3   \n",
       "71    SPS-DISTRICT K-12                  K-12 SCHOOL                    6   \n",
       "\n",
       "          Neighborhood  YearBuilt  NumberofFloors  PropertyGFATotal  \\\n",
       "1966  GREATER DUWAMISH       1992             1.0      29394.000000   \n",
       "1091             NORTH       2011             1.0      51400.000000   \n",
       "1468              EAST       1985            10.0      42250.300000   \n",
       "438           DOWNTOWN       1971             9.0      24984.111111   \n",
       "1790         NORTHWEST       1962             1.0      22047.000000   \n",
       "...                ...        ...             ...               ...   \n",
       "1314          DOWNTOWN       1973            19.0      17477.210526   \n",
       "930          SOUTHEAST       1967             2.0      11625.000000   \n",
       "1735          DOWNTOWN       1907             6.0       5866.666667   \n",
       "59           SOUTHEAST       2003             3.0      33144.333333   \n",
       "71             BALLARD       1991             2.0      27756.500000   \n",
       "\n",
       "      PropertyGFABuilding(s) SecondLargestPropertyUseType  ENERGYSTARScore  \\\n",
       "1966              100.000000                       OFFICE             60.0   \n",
       "1091              100.000000                      PARKING             75.0   \n",
       "1468               51.105673               MEDICAL OFFICE             46.0   \n",
       "438               100.000000                 LARGE OFFICE             73.0   \n",
       "1790              100.000000                      PARKING            100.0   \n",
       "...                      ...                          ...              ...   \n",
       "1314               82.148181                        HOTEL              6.0   \n",
       "930               100.000000  SMALL- AND MID-SIZED OFFICE             51.0   \n",
       "1735              100.000000  SMALL- AND MID-SIZED OFFICE              8.0   \n",
       "59                 64.044130                      PARKING             71.0   \n",
       "71                100.000000                  K-12 SCHOOL             91.0   \n",
       "\n",
       "      SteamUse(kBtu)  NaturalGas(kBtu)  \n",
       "1966               0                 1  \n",
       "1091               0                 1  \n",
       "1468               1                 0  \n",
       "438                1                 0  \n",
       "1790               0                 0  \n",
       "...              ...               ...  \n",
       "1314               1                 1  \n",
       "930                0                 0  \n",
       "1735               0                 0  \n",
       "59                 0                 0  \n",
       "71                 0                 0  \n",
       "\n",
       "[441 rows x 12 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BuildingType</th>\n",
       "      <th>PrimaryPropertyType</th>\n",
       "      <th>CouncilDistrictCode</th>\n",
       "      <th>Neighborhood</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>NumberofFloors</th>\n",
       "      <th>PropertyGFATotal</th>\n",
       "      <th>PropertyGFABuilding(s)</th>\n",
       "      <th>SecondLargestPropertyUseType</th>\n",
       "      <th>ENERGYSTARScore</th>\n",
       "      <th>SteamUse(kBtu)</th>\n",
       "      <th>NaturalGas(kBtu)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1966</th>\n",
       "      <td>NONRESIDENTIAL</td>\n",
       "      <td>WAREHOUSE</td>\n",
       "      <td>2</td>\n",
       "      <td>GREATER DUWAMISH</td>\n",
       "      <td>1992</td>\n",
       "      <td>1.0</td>\n",
       "      <td>29394.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>OFFICE</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1091</th>\n",
       "      <td>NONRESIDENTIAL</td>\n",
       "      <td>SUPERMARKET / GROCERY STORE</td>\n",
       "      <td>5</td>\n",
       "      <td>NORTH</td>\n",
       "      <td>2011</td>\n",
       "      <td>1.0</td>\n",
       "      <td>51400.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>PARKING</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1468</th>\n",
       "      <td>NONRESIDENTIAL</td>\n",
       "      <td>MEDICAL OFFICE</td>\n",
       "      <td>3</td>\n",
       "      <td>EAST</td>\n",
       "      <td>1985</td>\n",
       "      <td>10.0</td>\n",
       "      <td>42250.300000</td>\n",
       "      <td>51.105673</td>\n",
       "      <td>MEDICAL OFFICE</td>\n",
       "      <td>46.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>NONRESIDENTIAL</td>\n",
       "      <td>LARGE OFFICE</td>\n",
       "      <td>7</td>\n",
       "      <td>DOWNTOWN</td>\n",
       "      <td>1971</td>\n",
       "      <td>9.0</td>\n",
       "      <td>24984.111111</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>LARGE OFFICE</td>\n",
       "      <td>73.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1790</th>\n",
       "      <td>NONRESIDENTIAL</td>\n",
       "      <td>WORSHIP FACILITY</td>\n",
       "      <td>5</td>\n",
       "      <td>NORTHWEST</td>\n",
       "      <td>1962</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22047.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>PARKING</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1314</th>\n",
       "      <td>NONRESIDENTIAL</td>\n",
       "      <td>HOTEL</td>\n",
       "      <td>7</td>\n",
       "      <td>DOWNTOWN</td>\n",
       "      <td>1973</td>\n",
       "      <td>19.0</td>\n",
       "      <td>17477.210526</td>\n",
       "      <td>82.148181</td>\n",
       "      <td>HOTEL</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>930</th>\n",
       "      <td>NONRESIDENTIAL</td>\n",
       "      <td>SMALL- AND MID-SIZED OFFICE</td>\n",
       "      <td>2</td>\n",
       "      <td>SOUTHEAST</td>\n",
       "      <td>1967</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11625.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>SMALL- AND MID-SIZED OFFICE</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1735</th>\n",
       "      <td>NONRESIDENTIAL</td>\n",
       "      <td>SMALL- AND MID-SIZED OFFICE</td>\n",
       "      <td>7</td>\n",
       "      <td>DOWNTOWN</td>\n",
       "      <td>1907</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5866.666667</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>SMALL- AND MID-SIZED OFFICE</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>NONRESIDENTIAL</td>\n",
       "      <td>SMALL- AND MID-SIZED OFFICE</td>\n",
       "      <td>3</td>\n",
       "      <td>SOUTHEAST</td>\n",
       "      <td>2003</td>\n",
       "      <td>3.0</td>\n",
       "      <td>33144.333333</td>\n",
       "      <td>64.044130</td>\n",
       "      <td>PARKING</td>\n",
       "      <td>71.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>SPS-DISTRICT K-12</td>\n",
       "      <td>K-12 SCHOOL</td>\n",
       "      <td>6</td>\n",
       "      <td>BALLARD</td>\n",
       "      <td>1991</td>\n",
       "      <td>2.0</td>\n",
       "      <td>27756.500000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>K-12 SCHOOL</td>\n",
       "      <td>91.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>441 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           BuildingType          PrimaryPropertyType  CouncilDistrictCode  \\\n",
       "1966     NONRESIDENTIAL                    WAREHOUSE                    2   \n",
       "1091     NONRESIDENTIAL  SUPERMARKET / GROCERY STORE                    5   \n",
       "1468     NONRESIDENTIAL               MEDICAL OFFICE                    3   \n",
       "438      NONRESIDENTIAL                 LARGE OFFICE                    7   \n",
       "1790     NONRESIDENTIAL             WORSHIP FACILITY                    5   \n",
       "...                 ...                          ...                  ...   \n",
       "1314     NONRESIDENTIAL                        HOTEL                    7   \n",
       "930      NONRESIDENTIAL  SMALL- AND MID-SIZED OFFICE                    2   \n",
       "1735     NONRESIDENTIAL  SMALL- AND MID-SIZED OFFICE                    7   \n",
       "59       NONRESIDENTIAL  SMALL- AND MID-SIZED OFFICE                    3   \n",
       "71    SPS-DISTRICT K-12                  K-12 SCHOOL                    6   \n",
       "\n",
       "          Neighborhood  YearBuilt  NumberofFloors  PropertyGFATotal  \\\n",
       "1966  GREATER DUWAMISH       1992             1.0      29394.000000   \n",
       "1091             NORTH       2011             1.0      51400.000000   \n",
       "1468              EAST       1985            10.0      42250.300000   \n",
       "438           DOWNTOWN       1971             9.0      24984.111111   \n",
       "1790         NORTHWEST       1962             1.0      22047.000000   \n",
       "...                ...        ...             ...               ...   \n",
       "1314          DOWNTOWN       1973            19.0      17477.210526   \n",
       "930          SOUTHEAST       1967             2.0      11625.000000   \n",
       "1735          DOWNTOWN       1907             6.0       5866.666667   \n",
       "59           SOUTHEAST       2003             3.0      33144.333333   \n",
       "71             BALLARD       1991             2.0      27756.500000   \n",
       "\n",
       "      PropertyGFABuilding(s) SecondLargestPropertyUseType  ENERGYSTARScore  \\\n",
       "1966              100.000000                       OFFICE             60.0   \n",
       "1091              100.000000                      PARKING             75.0   \n",
       "1468               51.105673               MEDICAL OFFICE             46.0   \n",
       "438               100.000000                 LARGE OFFICE             73.0   \n",
       "1790              100.000000                      PARKING            100.0   \n",
       "...                      ...                          ...              ...   \n",
       "1314               82.148181                        HOTEL              6.0   \n",
       "930               100.000000  SMALL- AND MID-SIZED OFFICE             51.0   \n",
       "1735              100.000000  SMALL- AND MID-SIZED OFFICE              8.0   \n",
       "59                 64.044130                      PARKING             71.0   \n",
       "71                100.000000                  K-12 SCHOOL             91.0   \n",
       "\n",
       "      SteamUse(kBtu)  NaturalGas(kBtu)  \n",
       "1966               0                 1  \n",
       "1091               0                 1  \n",
       "1468               1                 0  \n",
       "438                1                 0  \n",
       "1790               0                 0  \n",
       "...              ...               ...  \n",
       "1314               1                 1  \n",
       "930                0                 0  \n",
       "1735               0                 0  \n",
       "59                 0                 0  \n",
       "71                 0                 0  \n",
       "\n",
       "[441 rows x 12 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On se demander si l'importance de la variable ENERGYSTARScore dans le modèle depend d'autre variable , notamment la plus importantes :'PrimaryPropertyType'.\n",
    "Pour cela ont calculera la valeurs de SHAP de notre modèles et on affichera :\n",
    "- Le summary plot du modèles\n",
    "- Le dépendance plot des variables ENERGYSTARScore et PrimaryPropertyType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transformation = pd.DataFrame(rfr_grid_cv.named_steps['preprocessor'].transform(X_test),columns=ordinal+numeric+binar )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 441 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eacc05b3f2914bb992c7a614239eac35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/441 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=1.185e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.664e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.664e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.014e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.495e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=5.211e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=5.413e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.513e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.461e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.309e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.449e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.402e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.231e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=5.930e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.899e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=2.960e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=2.352e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.480e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.411e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.304e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=9.074e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=8.476e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=4.918e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=4.689e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=4.153e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.400e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.962e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.933e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.933e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.144e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.134e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=9.995e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=9.949e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=9.809e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=9.780e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=9.780e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=4.776e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=4.744e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=4.722e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=4.717e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=5.576e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=2.826e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=6.787e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=6.174e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=5.384e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.729e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=5.433e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=5.433e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=5.197e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=3.730e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.756e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.756e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.755e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=8.728e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=8.721e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=8.721e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=8.564e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.642e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.428e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.428e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.427e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.385e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=2.391e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=1.196e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=1.196e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=4.937e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=4.937e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.018e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.018e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=1.651e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.898e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.426e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.362e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=6.171e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.059e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.743e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.204e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=7.231e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.221e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.136e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=3.385e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=3.699e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.358e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=8.203e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.634e-06, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.634e-06, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.558e-06, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=9.415e-07, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=9.207e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.536e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.518e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.246e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.221e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.402e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=6.797e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=3.399e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=3.399e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=3.399e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.467e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.467e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.467e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.451e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.389e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.374e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.420e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.420e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.420e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.383e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=3.285e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.845e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.832e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.802e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 7 iterations, alpha=6.821e-03, previous alpha=2.826e-03, with an active set of 6 regressors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=6.165e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=3.082e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.415e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.386e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.748e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.110e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.110e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=7.873e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=7.792e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=5.932e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=5.881e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=1.838e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=9.191e-04, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=7.505e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.207e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.731e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.786e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.196e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.179e-06, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=4.120e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=8.127e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=7.868e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 4.344e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=7.817e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 11 iterations, alpha=6.223e-03, previous alpha=3.984e-03, with an active set of 6 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=4.925e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.463e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.463e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.455e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.229e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.186e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.186e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.623e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.578e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.578e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.573e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=5.637e-06, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=2.364e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.085e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.042e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.042e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.034e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=7.627e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=7.123e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.522e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.522e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.493e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.407e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=5.580e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=5.258e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.003e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.003e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.233e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.871e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.366e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=9.326e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.026e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.634e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.634e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.196e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.167e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=3.550e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.450e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.204e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.606e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.606e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=8.975e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=8.975e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=4.061e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.913e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.913e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=6.069e-04, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.439e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.439e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.964e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=1.493e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=1.493e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=7.465e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=7.465e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.317e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.317e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.042e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 6 iterations, alpha=3.069e-03, previous alpha=3.042e-03, with an active set of 5 regressors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=3.156e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=3.156e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.578e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.230e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=5.918e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=5.226e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.845e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.845e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=2.598e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=2.598e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.299e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.299e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.299e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=5.224e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=5.224e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=5.224e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=7.047e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.688e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.685e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.600e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.111e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.004e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.569e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=2.312e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=4.600e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=4.250e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.134e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.784e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.767e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=9.666e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=9.230e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.729e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.941e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.416e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=4.707e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=2.354e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=9.767e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.598e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=1.291e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=6.108e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=6.097e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=3.967e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=3.894e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.198e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=3.143e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 4.829e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.095e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=6.408e-03, with an active set of 1 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=4.814e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=2.407e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.094e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=7.551e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=5.187e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.936e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=9.209e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=2.488e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.244e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.244e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.236e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=9.825e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=5.259e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.970e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.535e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.204e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.187e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.129e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.129e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.107e-05, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.554e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.554e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.419e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=9.585e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=9.585e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=8.571e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.371e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.371e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.015e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=1.322e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=6.611e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=3.763e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.877e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.116e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.625e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.333e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=5.281e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.153e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.872e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.256e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.013e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.013e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=5.283e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.347e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.980e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.802e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.598e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.171e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=8.193e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=6.180e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=2.613e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.311e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=9.149e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.719e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.450e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.803e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.372e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.793e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.101e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.131e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=4.887e-06, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=8.774e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=8.774e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=8.686e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=5.033e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.370e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=8.044e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.158e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.043e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.271e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.118e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.318e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=9.703e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.851e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.851e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.606e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 7 iterations, alpha=3.197e-03, previous alpha=2.074e-03, with an active set of 6 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=9.497e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=4.749e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.969e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.927e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.837e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.723e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.695e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.695e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 7 iterations, alpha=1.484e-03, previous alpha=1.181e-03, with an active set of 6 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=6.715e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=5.140e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=4.984e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=4.972e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=1.354e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=9.260e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=6.772e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=6.772e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.425e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.863e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=9.366e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.733e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.733e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.485e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.707e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=2.349e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.174e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.174e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.158e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.151e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.112e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.262e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.262e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.204e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.178e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.394e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.579e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.579e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.558e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.548e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.525e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=5.494e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=5.494e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.423e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.423e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=8.032e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=6.191e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.936e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.733e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.287e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=4.313e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=4.313e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.202e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.336e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.395e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=5.490e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.745e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.610e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.163e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.084e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.365e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.297e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.201e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.201e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=5.566e-05, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.783e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.509e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.235e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.023e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=8.637e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=8.637e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.318e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.318e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.078e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.837e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.216e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.216e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.149e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=9.775e-06, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=9.775e-06, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=9.763e-06, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=9.720e-06, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=8.496e-06, with an active set of 7 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.670e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.335e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.335e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=1.450e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=7.252e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=7.252e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.431e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.431e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.944e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.944e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=9.002e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.501e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.501e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.528e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=6.644e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=6.517e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=6.466e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=6.466e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.322e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=3.185e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=3.184e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=3.147e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=3.200e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.737e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.737e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=4.561e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.745e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.582e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=3.509e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.337e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=5.328e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.382e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.382e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.140e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=5.700e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=5.700e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=4.337e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.144e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.144e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.824e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.492e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=1.952e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=1.952e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=9.760e-04, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=6.758e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=6.588e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.525e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.580e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.737e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=9.113e-05, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=7.129e-05, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=7.129e-05, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.895e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.384e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=9.468e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=9.327e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=9.282e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=6.309e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=6.078e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=2.019e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.992e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=5.455e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.851e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.171e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.071e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=9.021e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=6.851e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=6.372e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=3.426e-03, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=2.801e-03, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=2.686e-03, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=2.067e-03, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=3.453e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=3.453e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.568e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.551e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.551e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.547e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=7.861e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=7.847e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=7.822e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=6.779e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=2.907e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.453e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.400e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=8.899e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.205e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.275e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.275e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=6.373e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.944e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.263e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=1.009e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=3.992e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=3.602e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.721e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.553e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.081e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.850e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.218e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.184e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.884e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.743e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.198e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.255e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=6.276e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=6.202e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=6.153e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.221e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.221e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.206e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.197e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.373e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.186e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.184e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.183e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.164e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=2.259e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=8.219e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=8.058e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=6.496e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=4.522e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=2.216e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.861e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.855e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.512e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=1.325e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.241e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=5.790e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=5.650e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=9.110e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.423e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.939e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.039e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.028e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.299e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=6.497e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.789e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.690e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.488e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.714e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=9.916e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=6.699e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=6.699e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.128e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.162e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=7.826e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.985e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.809e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.621e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.202e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=6.055e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=3.735e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.782e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.746e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.742e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.373e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=7.892e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=7.790e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=8.697e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=1.832e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=9.162e-04, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=9.162e-04, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=3.504e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=3.504e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.995e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.966e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.099e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=4.423e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.168e-06, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=1.966e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.781e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=9.828e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=9.828e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=7.981e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.631e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.631e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.504e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.046e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.025e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.999e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.947e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.940e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.117e-06, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=1.851e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=9.253e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.005e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.368e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=1.517e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=7.585e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=5.924e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=5.011e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=4.743e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.085e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.085e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=8.473e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.593e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.486e-05, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=7.428e-06, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=7.428e-06, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=7.399e-06, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=7.921e-03, with an active set of 1 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=3.960e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=3.920e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=2.479e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=2.236e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.698e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.698e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=7.677e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.250e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.640e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.640e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=9.610e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=9.610e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.299e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=9.988e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=9.988e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.991e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.862e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.071e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.765e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=3.124e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.449e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=7.136e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=7.136e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=7.115e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=6.653e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=5.349e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=2.799e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=5.627e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.813e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.813e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.923e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.100e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=7.688e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=7.688e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=7.688e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.648e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=9.119e-04, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=4.735e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.279e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.736e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.028e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=7.851e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=6.224e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.660e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.765e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.757e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.754e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.633e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=8.378e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=4.189e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=4.189e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=4.189e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.079e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.079e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.079e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.446e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.163e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.162e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.121e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.674e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.672e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.614e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.104e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=7.874e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=5.121e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=5.121e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=2.552e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=2.068e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.076e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=7.417e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=4.085e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=8.648e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=4.133e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=2.901e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.104e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=5.519e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=5.519e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=5.051e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=4.377e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.371e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.371e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.567e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.605e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.227e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.219e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.164e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=7.791e-06, with an active set of 6 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=6.939e-06, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=6.057e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=6.057e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=5.876e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=4.987e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=6.072e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=6.072e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.999e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.107e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.107e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.735e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=3.816e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=3.122e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.498e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.892e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.843e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.214e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=8.651e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.497e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.289e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.289e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=7.007e-03, with an active set of 1 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=3.504e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.578e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.578e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.398e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.259e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 5 iterations, alpha=1.324e-03, previous alpha=1.252e-03, with an active set of 4 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.535e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=9.131e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=9.131e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.039e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.039e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.039e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.644e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=8.165e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=3.884e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.794e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.692e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=9.489e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=7.860e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.384e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.926e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.349e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=7.210e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=5.647e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=5.647e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=4.569e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=3.748e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.284e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.284e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.141e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.455e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=8.486e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=5.271e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=2.539e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.627e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.627e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.422e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=1.100e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=5.374e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.687e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.687e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.512e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.752e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.160e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.160e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.695e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.695e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=5.345e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.656e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.656e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.623e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 7 iterations, alpha=2.026e-02, previous alpha=1.375e-02, with an active set of 6 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=5.327e-05, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=5.327e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=5.327e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=5.163e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.285e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=2.655e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.523e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.082e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=9.173e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=7.748e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=7.748e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.172e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.415e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.628e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=7.100e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=6.200e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=6.200e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.934e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.562e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.562e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=2.585e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=8.820e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=8.820e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.174e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.073e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.775e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=1.278e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=5.943e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=4.960e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=4.424e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=3.014e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=8.454e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.674e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.674e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.669e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=1.207e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=6.036e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=5.280e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.693e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.896e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.659e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.659e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.065e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.340e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=6.837e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=4.707e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=4.707e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.525e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.525e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.093e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=6.443e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=6.443e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=6.371e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=6.302e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=2.351e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=1.175e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=1.175e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=4.630e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=4.630e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.840e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.840e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.441e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.215e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.194e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.174e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=9.903e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.476e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.476e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.463e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.203e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=5.867e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=5.867e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=5.817e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=4.781e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=4.561e-06, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=3.428e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.714e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.699e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.686e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.684e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.226e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.211e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=4.328e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=3.340e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.164e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.125e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.328e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.230e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.516e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.489e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=2.816e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=1.440e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.157e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=5.656e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=5.641e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.684e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.151e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.147e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.601e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.197e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.197e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.105e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.048e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.882e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.769e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.129e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=2.309e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.295e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.090e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.072e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=7.880e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.064e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.193e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.322e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.346e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=4.927e-06, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=4.874e-06, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=4.874e-06, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.975e-06, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=1.906e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.429e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.275e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.595e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.595e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.214e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.680e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.680e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.124e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.069e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.517e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.517e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.293e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.147e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.147e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.402e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.402e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.254e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.138e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.019e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.724e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.724e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.715e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.712e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=3.462e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.731e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.420e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=2.127e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=2.127e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.063e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.057e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 6 iterations, alpha=9.491e-03, previous alpha=9.146e-03, with an active set of 5 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=9.487e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=4.744e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=4.744e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.164e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.164e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.431e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.037e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.037e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.942e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.200e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=7.558e-05, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=1.658e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=8.291e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=8.140e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.292e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.288e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 4.081e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.001e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.001e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.708e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.913e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 4.081e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.537e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 4.081e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=6.503e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=6.503e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=6.499e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=4.489e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=1.401e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=1.401e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=7.006e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=7.006e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=6.941e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=6.941e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.976e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.976e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.948e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.352e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.352e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.339e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.185e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.181e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.150e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.149e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=1.973e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=9.866e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=9.866e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=5.571e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.826e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.445e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.550e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=7.502e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=2.952e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.730e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.100e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.100e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=8.113e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.794e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.938e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.350e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=9.850e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=4.925e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=4.897e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=4.869e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=4.843e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.013e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.521e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.110e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.810e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.668e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.141e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.141e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.141e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.126e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.001e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.001e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=7.791e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=9.712e-05, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=3.124e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.221e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.188e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=5.385e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.416e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.498e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.314e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.380e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.224e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.294e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=9.465e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=9.296e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.996e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.817e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.785e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=1.054e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.688e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.688e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.678e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.472e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=6.504e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=6.504e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=6.478e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=4.117e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=1.442e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.376e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=7.209e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=7.075e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=7.075e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=6.554e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.044e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.988e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.871e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=7.490e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=7.351e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.315e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=5.509e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=5.509e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.902e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.902e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.423e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.423e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.582e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=7.912e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=7.858e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=7.367e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.875e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.849e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.829e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.431e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=9.890e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=9.890e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=9.890e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=9.756e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=9.696e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.424e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.424e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.424e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.378e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.357e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.000e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.541e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.541e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.115e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.212e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.737e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=6.075e-05, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.037e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=5.038e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.793e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.238e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.062e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.440e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.034e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=4.304e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=4.304e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=4.293e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=4.250e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.608e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.608e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.604e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.588e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=3.526e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=3.160e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=3.160e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.505e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=8.178e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=6.662e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=6.662e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=5.282e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.969e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.417e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=7.086e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=7.086e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=7.075e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=7.075e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=7.064e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.739e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.739e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.735e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.735e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=2.003e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.855e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 6 iterations, alpha=1.583e-03, previous alpha=1.546e-03, with an active set of 5 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=1.009e-01, with an active set of 1 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.237e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.234e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.191e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.999e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.450e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.073e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.072e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.058e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=8.204e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.560e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=7.804e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=7.799e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=7.796e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=3.350e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=3.348e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=2.422e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.286e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=8.374e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=4.830e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=4.016e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=4.014e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.252e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=2.126e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=2.126e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.665e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.665e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.204e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.791e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.791e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.791e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.204e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=5.874e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=5.874e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=5.874e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.539e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.516e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.991e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.991e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.991e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=4.013e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.006e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.951e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.880e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.965e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.775e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=5.705e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=8.644e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.924e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.876e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.263e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.467e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=1.512e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=5.310e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=4.456e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=2.403e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=2.053e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=2.015e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.310e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=3.211e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.541e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.483e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.428e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=7.298e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=5.442e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=5.206e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.258e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=7.288e-03, with an active set of 1 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=3.678e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=3.610e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.026e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.520e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.374e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.526e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=8.855e-05, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=7.062e-05, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=6.105e-05, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.318e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.407e-06, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.407e-06, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.081e-06, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=1.944e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=1.944e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=8.646e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=8.646e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=6.801e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.705e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.705e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.021e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.429e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=7.147e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=7.071e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.091e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.058e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.438e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=2.723e-03, with an active set of 1 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.853e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.853e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.026e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.013e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.013e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=8.433e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=8.433e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=7.390e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=7.215e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.764e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=4.317e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=4.317e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.884e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.483e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.028e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=4.424e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=4.306e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=4.112e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=4.112e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.229e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=1.405e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=5.533e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.693e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.974e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.673e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.590e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.013e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.840e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=7.507e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=5.123e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=5.123e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=5.123e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=2.565e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=8.271e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=6.788e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=6.788e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 6 iterations, alpha=6.767e-03, previous alpha=6.040e-03, with an active set of 5 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=1.151e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.716e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.640e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.594e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 6 iterations, alpha=3.712e-03, previous alpha=2.522e-03, with an active set of 5 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=1.030e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=8.823e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=6.774e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 5 iterations, alpha=7.072e-03, previous alpha=4.410e-03, with an active set of 4 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=7.027e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.510e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.504e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.496e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.492e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.485e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.786e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=8.931e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.770e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.311e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.311e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.146e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.648e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=8.484e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=8.371e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=8.371e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=5.271e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=4.186e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=3.212e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.085e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=9.318e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.704e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.704e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=1.456e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.213e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=6.121e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=6.121e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=5.752e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=5.566e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.920e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.014e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.394e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.394e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.172e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 7 iterations, alpha=3.832e-03, previous alpha=2.977e-04, with an active set of 6 regressors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=6.390e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.376e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.349e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=5.885e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=5.129e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.354e-06, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.237e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=4.006e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.533e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.399e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.360e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=7.687e-05, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=5.659e-05, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.302e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.617e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.274e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.274e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.294e-06, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=3.210e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.627e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.597e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.479e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.063e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=8.958e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.506e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.259e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=8.689e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=8.663e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=8.592e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=3.824e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=1.912e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=1.776e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.324e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.324e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=8.280e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=5.031e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=5.031e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.509e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.509e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.124e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.124e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.301e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=5.806e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.082e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=7.341e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=6.970e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=4.450e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.395e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.355e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=6.775e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=6.675e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=6.668e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=6.176e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=8.409e-03, with an active set of 1 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=4.027e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=4.027e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=3.673e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=5.465e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.101e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.101e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.488e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.466e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.009e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.997e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.997e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.906e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=2.784e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.245e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.219e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=9.244e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=8.119e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.572e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.786e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.786e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.749e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.165e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.167e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=1.001e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=6.056e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=6.044e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=6.044e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=6.034e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=5.300e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.873e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.883e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.883e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.869e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.857e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=1.294e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=7.497e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=5.658e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=2.150e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.961e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.026e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=9.351e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=9.351e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=9.127e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.429e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.429e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.437e-05, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.931e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.931e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.931e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.255e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.255e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.255e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.696e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=2.439e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.165e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.138e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=5.556e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=5.512e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=5.512e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.708e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=2.626e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.559e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.779e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.347e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.611e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=5.459e-05, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.462e-05, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.802e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=9.046e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=9.046e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=8.969e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=9.494e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=9.494e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.421e-06, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.393e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.696e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.696e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=2.705e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=2.277e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.331e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.265e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=9.294e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.394e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.384e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.380e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.374e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.154e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=6.170e-04, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=3.085e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.984e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.883e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.196e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.157e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.054e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=9.839e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=9.106e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=4.280e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=4.262e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=4.122e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=4.080e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=6.574e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=6.226e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.113e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.113e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.729e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=8.645e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=8.645e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=8.645e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.921e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=4.404e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=4.404e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=4.338e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=4.338e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=4.082e-06, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=5.354e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.490e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.571e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.571e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.918e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.918e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.497e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.097e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.090e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=5.450e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=5.450e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.010e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=1.222e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=6.110e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=3.234e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.734e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.734e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=5.000e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=2.135e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=1.824e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=9.394e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=9.394e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=7.796e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.423e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=9.772e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=8.814e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=7.859e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.431e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=9.104e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=7.190e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=3.141e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.570e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.558e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.983e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.926e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=7.155e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=4.293e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.938e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=8.883e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=7.203e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.976e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.936e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=7.614e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.807e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.795e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.666e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.056e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.127e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.121e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.839e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.830e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.733e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=1.049e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=5.245e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=2.212e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.009e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.532e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=1.318e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=6.589e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.210e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.366e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.295e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.811e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.327e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.386e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.386e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.193e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.193e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.238e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.238e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=7.273e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=4.365e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.321e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=8.803e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=8.803e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=5.754e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.599e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=8.452e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=8.452e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=6.826e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=6.683e-06, with an active set of 7 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=1.264e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=4.919e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.460e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.620e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.978e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=5.088e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=4.727e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=4.727e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.799e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.799e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.518e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.333e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=2.511e-03, with an active set of 1 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=2.486e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=2.486e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=8.797e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=8.797e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=8.797e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.829e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.902e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.902e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.902e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.219e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.432e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.432e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.432e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.573e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.287e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.287e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=7.858e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=7.095e-03, with an active set of 1 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=3.548e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.258e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=8.873e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=5.765e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=6.298e-06, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.863e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.422e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.422e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.422e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.385e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.125e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.125e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.125e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.116e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=9.049e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=6.831e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=6.831e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=5.550e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=3.884e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.427e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.666e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.666e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.666e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.339e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.475e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.475e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.475e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.948e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.169e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=9.680e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=9.680e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=9.680e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=8.539e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=5.763e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.954e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.860e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.432e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.085e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=3.066e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.533e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.166e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.166e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=7.642e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=7.001e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.396e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=7.734e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.279e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=9.304e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=7.172e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.883e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.323e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=5.311e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=2.523e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.585e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=9.881e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=9.380e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=9.213e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.605e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.301e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=5.675e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.161e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 5.576e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.161e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.158e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=9.909e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=9.358e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.482e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 5.576e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.010e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.010e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=1.035e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=7.773e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=4.246e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.008e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.467e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=5.351e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.507e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.770e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.210e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=2.606e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.303e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=7.753e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.590e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.330e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.308e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=2.307e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.154e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=5.740e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=5.331e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.472e-05, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.381e-05, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.386e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.678e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.678e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.675e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.618e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.494e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.494e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.491e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.467e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=5.339e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=7.205e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=6.311e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=5.832e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.313e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.998e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.464e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.133e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.076e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.061e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=4.555e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=4.555e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=2.446e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=2.324e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.794e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=1.662e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=1.156e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=8.312e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=5.287e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=5.040e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=5.013e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.040e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=4.468e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.234e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.410e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=6.379e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=2.904e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.299e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.192e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.184e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 7 iterations, alpha=1.272e-02, previous alpha=1.120e-02, with an active set of 6 regressors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=9.299e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.649e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.589e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.079e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.589e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.725e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.930e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=3.306e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.653e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=6.272e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=5.673e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=3.795e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=5.395e-05, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=7.154e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=3.577e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=3.567e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.293e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.187e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.101e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.868e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=2.464e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=9.941e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=7.821e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=5.187e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 7 iterations, alpha=9.049e-03, previous alpha=1.880e-03, with an active set of 6 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=2.703e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.721e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.158e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=8.607e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 6 iterations, alpha=8.437e-03, previous alpha=5.433e-03, with an active set of 5 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=4.145e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.667e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.937e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.795e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=6.876e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=6.876e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.773e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.183e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.466e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=3.600e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.289e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.289e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.311e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.789e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.789e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.075e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.982e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.982e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.976e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.343e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.534e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.553e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.434e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=8.237e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.518e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.518e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.518e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.264e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.119e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.119e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.061e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.006e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=7.563e-03, with an active set of 1 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=3.782e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=3.782e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.699e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.291e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.291e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.310e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.310e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.283e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.283e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=3.936e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=3.730e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.575e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.132e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=8.055e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=6.771e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=7.916e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=5.618e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=5.519e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=5.519e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.809e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.809e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.710e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.710e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.213e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.213e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.213e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.785e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.785e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.785e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=3.642e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.228e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.195e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 3.799e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.147e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 3.799e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.540e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.053e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=5.200e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.561e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.130e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.057e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=5.818e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.977e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.977e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.338e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=8.502e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=4.251e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=4.251e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=3.224e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.033e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.033e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.033e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=8.682e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.585e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.585e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.585e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.290e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.289e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.168e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.524e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.343e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.297e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.135e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=5.932e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.402e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=6.649e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=6.649e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=6.583e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 7 iterations, alpha=6.474e-03, previous alpha=4.626e-03, with an active set of 6 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.368e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.184e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.184e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=9.536e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=9.199e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=6.308e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=1.821e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=9.107e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=7.127e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.176e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.311e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.283e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=9.275e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=9.089e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=6.507e-03, with an active set of 1 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=3.253e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=3.253e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=2.336e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=9.973e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=9.973e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.170e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.123e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.259e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.259e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=2.498e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.835e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.334e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=6.063e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=5.370e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=5.370e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=7.544e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=2.291e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=9.846e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=5.256e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=6.605e-06, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=5.775e-06, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=5.849e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=5.849e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.733e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.733e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.956e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.110e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.110e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=7.414e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.260e-05, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.130e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.130e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.091e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=1.064e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=7.441e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=7.441e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.736e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.325e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.916e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.358e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.229e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=8.198e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=8.198e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=7.286e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.088e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.633e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=1.681e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=8.407e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=8.024e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=5.360e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=5.056e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.090e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.995e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=6.281e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.360e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.826e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.742e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.838e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.829e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.700e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.700e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=4.156e-03, with an active set of 1 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=2.078e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.535e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.173e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=8.854e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.406e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.406e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.311e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.048e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=6.661e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.783e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.706e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.706e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.706e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=5.829e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=5.829e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=5.813e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=2.560e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=2.087e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=2.087e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=2.087e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=2.081e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=3.565e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.977e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.588e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=4.571e-04, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.286e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.286e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.613e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=7.378e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=7.378e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=1.387e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.975e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.515e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.085e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.945e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=9.352e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=9.352e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=8.272e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=8.035e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.475e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.238e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.233e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.233e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=7.802e-03, with an active set of 1 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=3.901e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.681e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.285e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.285e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=8.836e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=8.592e-06, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=5.906e-06, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=9.891e-03, with an active set of 1 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=4.316e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=1.725e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=3.102e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.453e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.292e-05, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.379e-05, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=6.267e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=3.134e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.213e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.104e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.748e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=8.182e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=8.120e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=8.107e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=7.987e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.975e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.945e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.923e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.918e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.911e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.875e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=1.348e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=6.741e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=6.741e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=6.675e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=6.666e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.371e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.371e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.162e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=2.027e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.228e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.083e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=9.465e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=9.465e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=2.757e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=5.943e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.699e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.623e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=9.212e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.882e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.882e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.955e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.842e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.842e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.842e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=2.110e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=8.542e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=8.466e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.611e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.579e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.973e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.973e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=9.180e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.879e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=9.101e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=8.913e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=5.529e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=5.529e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=9.120e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=9.120e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=8.452e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=5.254e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.465e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.533e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.288e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=8.256e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=5.229e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.691e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.048e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.048e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.701e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.638e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.477e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.077e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.728e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=9.154e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.577e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.577e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.664e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.177e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=1.838e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=9.191e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=8.975e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=4.595e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=4.595e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=3.186e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=2.113e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=1.056e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=5.404e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=3.501e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=7.919e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.793e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.327e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.480e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.480e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.480e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.285e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.248e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=7.737e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=7.402e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.048e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.048e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.959e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.959e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=7.494e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=7.494e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=7.169e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=4.145e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=6.640e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=5.718e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.398e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.807e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.419e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.419e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.028e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=7.307e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.047e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.315e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.249e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.181e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=5.796e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.641e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.342e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 3.799e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=1.149e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.848e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.393e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.319e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.255e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=6.566e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=6.207e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=1.862e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=7.966e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=6.875e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.839e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.347e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.196e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=8.330e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.839e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=1.435e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=7.176e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.288e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.420e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=2.984e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.008e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.008e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=9.825e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.832e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=1.402e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.961e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.933e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.184e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.319e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.470e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.437e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=6.711e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.356e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.356e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.860e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.860e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.533e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.379e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.510e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.021e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=5.106e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=5.106e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.452e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.452e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.353e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.203e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.887e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.150e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.150e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.268e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.733e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.903e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.458e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.458e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=7.821e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.169e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.378e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.378e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.101e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.101e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=5.871e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=9.371e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=4.685e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=4.050e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=3.415e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.599e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.383e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=9.406e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=2.322e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=2.220e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.043e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=9.969e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=9.543e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=4.226e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=4.028e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.850e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.850e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=6.872e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=3.424e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=3.400e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.653e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.627e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.420e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.333e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=5.267e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=5.267e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=5.216e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=5.087e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=8.249e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=8.249e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=4.125e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=4.125e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.878e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.878e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=8.480e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=8.480e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=8.480e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=5.607e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=3.797e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.898e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.898e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.898e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.707e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.347e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.347e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.347e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.080e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=8.646e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=5.402e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.224e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.268e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.066e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=9.798e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=1.311e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.052e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.876e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.769e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.622e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.134e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.020e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=1.068e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=4.340e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=4.042e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=4.042e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=3.977e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=3.030e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.960e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=8.074e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=8.074e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=7.944e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.024e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.024e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.975e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.833e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=1.223e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=6.092e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=6.092e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=6.048e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=6.004e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.785e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.785e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.765e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=7.138e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=7.138e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=7.086e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.308e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=6.580e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=6.580e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=6.502e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=6.425e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=6.397e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=6.379e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=2.582e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=2.575e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=2.545e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=2.545e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=2.515e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=5.481e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=2.740e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=2.511e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.506e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.264e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=6.681e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=5.988e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=5.988e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=5.584e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=5.330e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.045e-06, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=3.336e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.668e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.619e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.569e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=7.722e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=7.492e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.949e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.949e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.891e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=6.062e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.029e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.029e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.020e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.384e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.382e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.312e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.307e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=6.039e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=6.031e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=5.927e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=5.919e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.943e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.278e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.832e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=2.405e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.202e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.201e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.201e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.200e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.160e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.045e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.041e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.904e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.805e-05, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.698e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.698e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=9.450e-07, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=1.034e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=5.170e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=3.194e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.712e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=8.927e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.247e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.177e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.502e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.415e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.646e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.791e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.791e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.791e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=3.402e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=1.906e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.701e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.459e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.391e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=7.781e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=7.781e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.093e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.112e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=5.636e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.024e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.104e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.104e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.946e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.784e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.264e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=1.253e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=7.012e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=5.139e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=5.139e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.883e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.181e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.085e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.085e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.085e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=8.089e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=2.472e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.757e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.196e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 6 iterations, alpha=1.577e-03, previous alpha=1.080e-03, with an active set of 5 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.527e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=7.255e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=5.283e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.160e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.218e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=7.587e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=9.274e-03, with an active set of 1 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=9.153e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.516e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.439e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.316e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 6 iterations, alpha=4.487e-03, previous alpha=3.635e-03, with an active set of 5 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.913e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.501e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=5.638e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 7 iterations, alpha=5.957e-04, previous alpha=5.422e-04, with an active set of 6 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=2.955e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.386e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.386e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=7.115e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.557e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.557e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.398e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.734e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.696e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.696e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.166e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=1.534e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=7.671e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=5.578e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.808e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.646e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.448e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.117e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.266e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=9.865e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=9.513e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.440e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.440e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=5.823e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.325e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.325e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.703e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.992e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.976e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.976e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=8.304e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=4.152e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=3.970e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.479e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.458e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.923e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.016e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.032e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.032e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.015e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.015e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.610e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.610e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.582e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.582e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=2.269e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=1.134e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=1.134e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.994e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.994e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.355e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.282e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.530e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.985e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.985e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.832e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.814e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.814e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.777e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.404e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.270e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.270e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.225e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.828e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.828e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.791e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=7.920e-04, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.664e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.664e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.271e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.054e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.447e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.002e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.247e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.090e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.058e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.056e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.022e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 8 iterations, alpha=1.080e-02, previous alpha=8.240e-03, with an active set of 7 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=2.421e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.194e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.753e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.416e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=7.078e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=7.078e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=6.655e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.996e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=9.978e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=9.971e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=8.871e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.435e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.435e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.432e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.432e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.880e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.880e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.878e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=2.330e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=2.129e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.165e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=9.646e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=4.684e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.070e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.402e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=7.012e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=7.012e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.081e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=9.392e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=7.149e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=7.149e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.161e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=1.450e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=1.088e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=9.990e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=6.486e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=4.529e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=4.359e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.440e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=7.571e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=7.571e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=7.571e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.535e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=2.016e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.093e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.008e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=8.429e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=8.413e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.694e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.552e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.480e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.208e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.356e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.305e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=2.192e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.782e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.022e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.013e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.013e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=8.741e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=7.856e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=7.856e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.575e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.575e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.552e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.552e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=9.990e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.995e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.995e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.498e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.498e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=9.186e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=8.539e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=4.240e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=4.157e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=4.123e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.634e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.611e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.602e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.490e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=9.603e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=9.603e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.675e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.228e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.625e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=7.018e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=7.018e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.544e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=4.081e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.658e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.782e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.028e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.992e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.083e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.068e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.333e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=6.663e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.330e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.204e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.709e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.126e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.126e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=3.850e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=8.043e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.765e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.245e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 7 iterations, alpha=8.024e-03, previous alpha=1.983e-03, with an active set of 6 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.122e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=8.032e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 3.799e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=7.852e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 3.799e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=7.841e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 3.799e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=7.765e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.523e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.521e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.521e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 3.799e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.506e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=1.024e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=5.524e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=4.432e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.495e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.788e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.788e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.717e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.051e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=6.661e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=6.661e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=6.661e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=6.069e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.895e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.448e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.448e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.448e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=1.435e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=7.175e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=6.630e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=6.067e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.027e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.868e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.258e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.923e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=1.708e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=6.199e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=5.963e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=5.893e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.334e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.758e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.296e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.056e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.139e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.108e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.096e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=3.598e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.084e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=9.834e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=9.644e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=6.694e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=4.923e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=4.450e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=4.450e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.288e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.022e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.009e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.008e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=2.439e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=2.439e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.220e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.220e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.270e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.270e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.087e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.487e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.487e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.487e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.486e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=5.556e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=5.556e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=5.552e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=5.552e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=2.021e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=2.021e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=2.019e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=2.019e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=7.144e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=3.572e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.467e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.892e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.823e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=5.969e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=5.900e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 540x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Récuperation de valeurs SHAP du modele\n",
    "plt.figure()\n",
    "explainer = shap.KernelExplainer(GHG_rfr_model.named_steps[\"grid_search_rfr\"].predict,data_transformation,feature_names=feature_names)\n",
    "shap_value=explainer.shap_values(data_transformation,nsamples=100)\n",
    "shap.dependence_plot(\"ENERGYSTARScore\",shap_value, data_transformation,interaction_index=2,show=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x453.6 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "shap.summary_plot(shap_value,data_transformation,show=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etude meilleur modele EUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rappel des meilleurs paramètres :\n",
      "{'bootstrap': False, 'max_depth': 25, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 2}\n"
     ]
    }
   ],
   "source": [
    "plot_search_results(EUI_rfr_model.named_steps['grid_search_rfr'], title=\"SourceEUI(kBtu/sf)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'évolution des résultats selon les hyperparamètres est exactement la meme que pour la variable TotalGHGEmission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Récuperation des SHAP values pour le modèle EUI_rfr_model\n",
    "result = permutation_importance(\n",
    "    EUI_rfr_model, X_test, Y_test[\"SourceEUI(kBtu/sf)\"], n_repeats=11, random_state=42, n_jobs=2\n",
    ")\n",
    "forest_importances = pd.Series(result.importances_mean, index=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(16,8))\n",
    "forest_importances.plot.bar(yerr=result.importances_std, ax=ax)\n",
    "ax.set_title(\"Feature importances using permutation on full model\")\n",
    "ax.set_ylabel(\"Mean accuracy decrease\")\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les variables ENERGYSTARScore et PrimaryPropertyType sont les plus importantes pour le modèle , cependant pour cette target le modèle accorde de l'importance a toute les variables du DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "result=X_test.copy()\n",
    "result['result']=EUI_xgb_model.predict(X_test)\n",
    "result['error']=abs(Y_test[\"SourceEUI(kBtu/sf)\"]-result['result'])\n",
    "sns.displot(x='error',data=result,bins=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['y']=Y_test[\"SourceEUI(kBtu/sf)\"]\n",
    "plt.figure(figsize=(16,8))\n",
    "sns.scatterplot(x='result',y=\"y\",data=result,hue='PrimaryPropertyType',)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toute nos erreur sont inférieur à 1 et concentré autour de zero, de plus on peut voir que les résultats sont bien regroupé autour de la première bissetrice, la prediction de la variable SourceEUI est donc plus précise que celle de la variable TotalGHEEmission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le modèle de RandomForest est le meilleur modèle pour la prediction des deux variables, cependant si les prédictions de la variable SourceEUI sont précises , celles des emissions donne des résultats très éloigné de la bissectrices et ne semble donc pas optimal.\n",
    "La variable ENERGYSTARScore n'apporte que très peu de précision lors de la prédiction des emissions et au vu de sa difficulté d'obtention, ne semble pas etre un investissement pertinent pour prédire cette donnée.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
